\section{Methods}
\label{sec:methods}

Given a dataset $\textbf{X} = \{x_1, \dots, x_n\}$ and a distance function $f : \textbf{X} \times \textbf{X} \mapsto \mathbb{R}^+ \ \cup \ \{0\}$, we define a \emph{cluster} $\mathcal{C}$ as a set of points with a \emph{center} and \emph{radius}.
The center $c$ is the geometric median of the points in the cluster (i.e., the point that minimizes the sum of distances to all other points in $\mathcal{C}$) and the radius $r$ is the maximum distance from $c$ to any point in $\mathcal{C}$.
Using previous work in CHESS~\cite{ishaq2019clustered}, and CLAM~\cite{ishaq2021clustered}, we build a binary tree $\mathcal{T}$ of clusters by recursively partitioning the dataset $\textbf{X}$ (See Alg.~\ref{alg:methods:partition}).
In practice, we approximate $c$ using a random sample of points from $\mathcal{C}$.
\begin{algorithm}[h] % enter the algorithm environment
    \caption{\textsc{Partition}($\mathcal{C}$, $criteria$)} % give the algorithm a caption
    \label{alg:methods:partition} % and a label for \ref{} commands later in the document
    \begin{algorithmic} % enter the algorithmic environment
        \Require $f$, a distance function
        \Require $\mathcal{C}$, a cluster
        \Require $criteria$, user-specified continuation criteria
        
        \If{$|C| > 1$ \textbf{and} $\mathcal{C}$ satisfies $criteria$}
        \State $c \Leftarrow$ geometric median of $\mathcal{C}$
        \State $l \Leftarrow \argmax f(c, x) \ \forall \ x \in C$
        \State $r \Leftarrow \argmax f(l, x) \ \forall \ x \in C$
        \State $L \Leftarrow \{x \ | \ x \in C \land f(l, x) \le f(r, x)\}$
        \State $R \Leftarrow \{x \ | \ x \in C \land f(r, x) < f(l, x)\}$
        \State \Call{Partition}{L, criteria}
        \State \Call{Partition}{R, criteria}
        \EndIf
    \end{algorithmic}
\end{algorithm}

The manifold hypothesis~\cite{fefferman2016testing} states that high($D$)-dimensional data collected from constrained generating phenomena occupy a low($d$)-dimensional manifold within their embedding space ($d \ll D$).
We define the \emph{local fractal dimension} (LFD) to quantify this property:
\begin{equation}
    \text{LFD}(p, r_1, r_2) = \frac{\text{log} \big( \frac{|B(p, r_1)|}{|B(p, r_2)|} \big) }{\text{log} \big( \frac{r_1}{r_2} \big) }
    \label{eq:methods:lfd-original}
\end{equation}
where $p \in X$ is a point and $B(p, r)$ is the set of points within radius $r$ from $p$.
When $d \ll D$ for the majority of points, we simply say that the dataset has low LFD.

\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{sec:methods:knn-search}

$k$-NN search in CAKES guarantees perfect recall when $f$ is a metric (i.e.\ it satisfies the triangle inequality) with performance that scales with LFD rather than $|\textbf{X}|$.

In the following, we use $\mathcal{Q}$ for the data structure containing clusters that are in contention for containing the $k$ nearest neighbors and $\mathcal{H}$ for a max-priority queue (of size $k$) containing the current $k$ nearest neighbors found.
We assume that the query $q$ is drawn from the same distribution as $X$, and we
denote the distances from $q$ to the center, the theoretically farthest, and the theoretically closest points in a cluster $\mathcal{C}$ using $\delta_\mathcal{C}$, $\delta^{+}_\mathcal{C}$, and $\delta^{-}_\mathcal{C}$, respectively.


\subsubsection{Repeated \texorpdfstring{$\rho$}{p}-NN}
\label{sec:methods:knn-search:repeated-rnn}

We begin by using \textsc{TreeSearch} on the root cluster $\mathcal{R}$ with the search radius $r$ equal to the radius of $\mathcal{R}$ divided by $|\textbf{X}|$.
If the returned set $\mathcal{Q}$ is empty, we double $r$ and repeat \textsc{TreeSearch} until $\mathcal{Q}$ is non-empty.
Now, so long as $\sum_{\mathcal{C} \in \mathcal{Q}} |\mathcal{C}| < k$, we repeat \textsc{TreeSearch}, but instead of doubling $r$, we multiply it by a factor determined by the LFD in the vicinity of the query ball.
In particular, we increase the radius by a factor of $\big( {\frac{k}{\sum_{\mathcal{C} \in \mathcal{Q}} |\mathcal{C}|}} \big)^\frac{1}{\mu}$
where $\mu$ is the harmonic mean of the LFD of the clusters in $\mathcal{Q}$ to ensure that this factor is not dominated by outlier clusters with very high LFD.
Intuitively, this factor will be large when we have found few points or the LFD is low because we need to increase the radius quickly, and small when we have found almost all $k$ points or LFD is large and we need to increase the radius slowly.
Once $\sum_{\mathcal{C} \in \mathcal{Q}} |\mathcal{C}| \geq k$, we perform \textsc{LeafSearch} to find the $k$ nearest neighbors.
\begin{algorithm}[h]
    \caption{\textsc{TreeSearch}($\mathcal{C}$, $q$, $r$)}
    \label{alg:methods:rnn-search:tree-search}
    \begin{algorithmic}
        \Require $\mathcal{C}$, a cluster
        \Require $q$, a query
        \Require $r$, a search radius
        \If{$\delta^+_C \leq r$}
            \State \textbf{return} $\{C\}$
        \EndIf
        \State $[L, R]$ $\Leftarrow$ \textit{children} of $\mathcal{C}$
        \State \textbf{return} \Call{TreeSearch}{$L, q, r$} $\cup$ \Call{TreeSearch}{$R, q, r$}
    \end{algorithmic}
\end{algorithm}
\vspace{-0.2in}

\begin{algorithm}[h]
    \caption{\textsc{LeafSearch}($\mathcal{Q}$, $q$, $r$)}
    \label{alg:methods:rnn-search:leaf-search}
    \begin{algorithmic}
        \Require $f$, a distance function
        \Require $\mathcal{Q}$, a set of clusters
        \Require $q$, a query
        \Require $r$, a search radius
        \State $H \Leftarrow \emptyset$
        \For{$C \in Q$}
            \For{$p \in C$ \textbf{if} $f(p, q) \leq r$}
                \State $\mathcal{H}$ $\Leftarrow$ $H \cup \{p\}$
            \EndFor
        \EndFor
        \State \textbf{return} $\mathcal{H}$
    \end{algorithmic}
\end{algorithm}
\vspace{-0.2in}


\subsubsection{Complexity of Repeated \texorpdfstring{$\rho$}{p}-NN}
\label{sec:methods:knn-search:repeated-rnn-complexity}

In CHESS~\cite{ishaq2019clustered}, we showed that the complexity of ranged search is
\begin{gather}
    \mathcal{O}
    \Bigg(
        \underbrace{
            \log~\overbrace{\mathcal{N}_{\hat{r}}(\textbf{X})}^{\textrm{metric entropy}}
        }_{\textrm{\textsc{TreeSearch}}}
        +
        \underbrace{
            \overbrace{ \big| B(q, \rho) \big|}^{\textrm{output size}}
            \overbrace{ \left( \frac{\rho + 2 \cdot \hat{r}}{ \rho} \right) ^ {\text{LFD}(q, \rho)}}^{\textrm{scaling factor}}
        }_{\textrm{\textsc{LeafSearch}}}
    \Bigg).
    \label{eq:methods:rnn-search-complexity}
\end{gather}

To extend this to Repeated $\rho$-NN, we first need to estimate the number of repetitions of \textsc{TreeSearch} needed to find a radius that guarantees at least one neighbor.
Since we expect $q$ to be from the same distribution as $\textbf{X}$, the LFD near $q$ should not differ significantly from the LFDs of clusters near $q$.
Thus, the multiplicative factor described above suggests that, in the expected case, we need only one iteration of \textsc{TreeSearch} to find the first cluster, and one more to find enough clusters to guarantee $k$ neighbors.
Since this is a constant factor, Repeated $\rho$-NN has the same complexity for \textsc{TreeSearch} as $\rho$-NN search, i.e.,\,$\mathcal{O}\big(\log\mathcal{N}_{\hat{r}}(\textbf{X})\big)$.

For \textsc{LeafSearch}, we must estimate $\sum_{\mathcal{C} \in \mathcal{Q}} |\mathcal{C}|$.
Since we must examine every point in each such cluster, the time complexity of \textsc{LeafSearch} is linear in this quantity.
Let $\rho_k$ be the distance from the query to the $k^{th}$ nearest neighbor.
Then, we see that $\mathcal{Q}$ is the set of clusters that overlap with a ball of radius $\rho_k$ around the query.
We can estimate this region as a ball of radius $\rho_k + 2\hat{r}$, where $\hat{r}$ is the mean radius of the clusters in $\mathcal{Q}$.
Thus, the \textsc{LeafSearch} component of Equation~\ref{eq:methods:rnn-search-complexity} becomes $k \cdot ( \frac{\rho_k + 2 \cdot \hat{r}}{\rho_k} )^{\hat{d}}$, where $\hat{d}$ is the harmonic mean of LFDs in $\mathcal{Q}$.
It remains to estimate $\rho_k$.

While ordinarily we would compute LFD by comparing cardinalities of two balls with two different radii centered at \textit{the same} point, in order to estimate $\rho_k$, we instead compare the cardinality of a ball \textit{around the query} of radius $\rho_k$ to the mean cardinality, $\hat{|\mathcal{C}|}$, of clusters in $\mathcal{Q}$ at a radius equal to the mean of their radii, $\hat{r}$.
Assuming that $q$ is from the same distribution as $\textbf{X}$, by Equation~\ref{eq:methods:lfd-original}, we have $\hat{d} = \frac{\log{}\frac{\hat{|\mathcal{C}|}}{k}}{\log{}\frac{\hat{r}}{\rho_k}}$, and rearranging, $\frac{\hat{r}}{\rho_k} = ( \frac{\hat{|\mathcal{C}|}}{k} )^{\hat{d}^{-1}}$.
This simplifies the \textsc{LeafSearch} component to $k ( 1 + 2 \cdot ( \frac{\hat{|\mathcal{C}|}}{k} ) ^ {\hat{d}^{-1}} )^d$.

Again using the assumption that $q$ is from the same distribution as $\textbf{X}$, we have that $\hat{d} \approx d$.
Thus, the asymptotic complexity of Repeated $\rho$-NN is
\begin{gather}
    \mathcal{O}
    \bigg(
        \underbrace{
            \log~\overbrace{\mathcal{N}_{\hat{r}}(\textbf{X})}^{\textrm{metric entropy}}
        }_{\textrm{\textsc{TreeSearch}}}
        +
        \underbrace{
            k \cdot
            \overbrace{\Big( 1 + 2 \cdot \Big( \frac{\hat{|\mathcal{C}|}}{k} \Big) ^ {d^{-1}} \Big)^d}^{\textrm{scaling factor}}
        }_{\textrm{\textsc{LeafSearch}}}
    \bigg).
    \label{eq:methods:repeated-rnn-complexity}
\end{gather}

We remark that the scaling factor in Equation~\ref{eq:methods:repeated-rnn-complexity} should be close to 1 unless LFD is highly variable in the region around the query (i.e.,\,if $\hat{d}$ differs significantly from $d$).


\subsubsection{Breadth-First Sieve}
\label{sec:methods:knn-search:bredth-first-sieve}

This algorithm (BFS) performs a breadth-first traversal of $\mathcal{T}$, pruning clusters by using a modified version of Hoare's QuickSelect algorithm~\cite{hoare1961algorithm}.

We begin by letting $\mathcal{Q}$ be a list of 3-tuples $(\mathcal{C}, \delta^{+}_{\mathcal{C}}, m)$, where $m$ is the multiplicity of $\mathcal{C}$ in $\mathcal{Q}$.
$\mathcal{Q}$ starts containing only $(\mathcal{R}, \delta^{+}_{\mathcal{R}}, |\mathcal{R}| - 1)$ while $\mathcal{H}$ starts with the center of $\mathcal{R}$, where $\mathcal{R}$ is the root of $\mathcal{T}$.
We then use QuickSelect, modified to account for multiplicities and to reorder $\mathcal{Q}$ in-place, to find $\tau$, the smallest $\delta^{+}$ in $\mathcal{Q}$ such that $\left| B(q, \tau) \right| \geq k$ (including the points in $\mathcal{H}$).
Since this step may require a binary search of $\mathcal{Q}$ and reordering $\mathcal{Q}$ takes linear time, this version of QuickSelect has $\mathcal{O}\left(|\mathcal{Q}| \log |\mathcal{Q}|\right)$ time complexity.

Next, we check every cluster in $\mathcal{Q}$, removing any for which $\delta^{-} > \tau$ because they cannot contain one of the $k$ nearest neighbors.
For each of the remaining clusters, if it is a leaf, we remove it from $\mathcal{Q}$ and add all its points to $\mathcal{H}$.
Otherwise (since it must be a parent cluster), we remove it from $\mathcal{Q}$ and, for each of its children, we compute $\delta^{+}$ and add the corresponding 3-tuple to $\mathcal{Q}$ and the child's center to $\mathcal{H}$, thus expanding the search to the next level in $\mathcal{T}$.
We continue until $\mathcal{Q}$ is empty or the farthest point in $\mathcal{H}$ is closer to $q$ than the $\delta^{-}$ of every cluster in $\mathcal{Q}$.
Thus, $\mathcal{H}$ contains the $k$ nearest neighbors to the query.

\subsubsection{Depth-First Sieve}
\label{sec:methods:knn-search:depth-first-sieve}

Let $\mathcal{Q}$ be a min-queue of clusters prioritized by $\delta^{-}$ starting with only $\mathcal{R}$ while $\mathcal{H}$ starts empty.
So long as $\mathcal{H}$ is not full or the top-priority items from $\mathcal{H}$ and $\mathcal{Q}$ have $\delta_{\mathcal{H}} \geq \delta_{\mathcal{Q}}$, we repeat following:
\begin{itemize}
    \item While the top-priority cluster is not a leaf, remove it from $\mathcal{Q}$ and add its children to $\mathcal{Q}$.
    \item Remove the top-priority cluster (a leaf) from $\mathcal{Q}$, add all its points to $\mathcal{H}$, and trim $\mathcal{H}$ to size $k$.
\end{itemize}
This process terminates when $|\mathcal{H}| = k$ and the top-priority items in $\mathcal{H}$ and $\mathcal{Q}$ are such that the next closest point to be visited in $\mathcal{Q}$ is farther from the query than the $k^{th}$ nearest neighbor found so far.
Thus, $\mathcal{H}$ contains the $k$ nearest neighbors to the query.


\subsubsection{Complexity of Sieve Methods}
\label{sec:methods:knn-search:complexity-of-sieve-methods}

Due to their similarity, we combine the analyses of BFS and DFS.

Let $d$ be the LFD in the region around $q$, and consider clusters with cardinalities near $k$.
Then the number of such clusters is bounded above by $2d$, where the bound is achieved by having a cluster overlap the query ball at each end of each of $\lceil d \rceil$ mutually-orthogonal axes.
In the worst-case scenario for \textsc{TreeSearch}, these clusters would all come from different branches of the tree, and \textsc{TreeSearch} would look at $2 \cdot \lceil d \rceil \cdot \log \mathcal{N}_{\hat{r}}(\textbf{X})$ clusters.
Thus, the asymptotic complexity of this component is $T \coloneqq \mathcal{O} \big( \lceil d \rceil \cdot \log \mathcal{N}_{\hat{r}}(\textbf{X}) \big)$.
For \textsc{LeafSearch}, the output size and scaling factor are the same as in Repeated $\rho$-NN, and so the complexity is $L \coloneqq \mathcal{O} \Big( k \cdot \Big( 1 + 2 \cdot \big( \frac{\hat{|\mathcal{C}|}}{k} \big) ^ {d^{-1}} \Big)^d \Big)$.

The asymptotic complexity of BFS is dominated by our modified QuickSelect algorithm, which accounts for the clusters in $\mathcal{Q}$ and the points in $\mathcal{H}$, and is log-linear in their sum.
Similarly, that of DFS is dominated by the priority queue operations for $T$ clusters and $L$ points.
These complexities are:
\begin{equation}
    \label{eq:methods:sieve-complexity}
    \begin{split}
    BFS \sim \ & \mathcal{O} \big( (T + L) \log(T + L) \big) \\
    DFS \sim \ & \mathcal{O} \big( T \log T + L \log L \big)
    \end{split}
\end{equation}
