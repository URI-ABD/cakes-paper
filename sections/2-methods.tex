\section{Methods}
\label{sec:methods}

In this manuscript, we study $k$-NN search in finite-dimensional spaces.
Given a dataset $\textbf{X} = \{x_1, \dots, x_n\}$, we refer to each $x_i \in \textbf{X}$ as a point.
Examples include neural embeddings, genomic sequences, and time-series.

We define a \textit{distance function} $f : \textbf{X} \times \textbf{X} \mapsto \mathbb{R}^+ \ \cup \ \{0\}$ which, given two points, deterministically returns a finite and non-negative real number.
We require $f$ to be symmetric ($f(x, y) = f(y, x)$) and identity-preserving ($f(x, y) = 0 \iff x = y$).
If $f$ satisfies the triangle inequality (i.e.,\,$f(x, y) \leq f(x, z) + f(z, y)$), then it is also a \textit{distance metric}.
Common examples include Euclidean, Levenshtein~\cite{levenshtein1966binary}, and DTW~\cite{muller2007dynamic};
Cosine distance violates the triangle inequality and is not a metric.
The appropriate distance function depends on the data: Euclidean or Cosine are often used for neural embeddings, Levenshtein or Hamming for sequences, and DTW or Wasserstein for time-series.
CAKES provides exact search under distance metrics.

CAKES assumes the manifold hypothesis~\cite{fefferman2016testing}, the notion that high($D$)-dimensional data collected from constrained generating phenomena often occupy a low($d$)-dimensional manifold within their embedding space ($d \ll D$).
We define the \emph{local fractal dimension} (LFD) to quantify this property.
For a point $q \in \textbf{X}$ and radii $r_1 > r_2$,
\begin{equation}
    \text{LFD}(q, r_1, r_2) = \frac{\text{log} \Big( \frac{|B(q, r_1)|}{|B(q, r_2)|} \Big) }{\text{log} \big( \frac{r_1}{r_2} \big) }
    \label{eq:methods:lfd-original}
\end{equation}
where $B(q, r)$ is the set of points in the metric ball of radius $r$ centered at $q$.
We typically use a simplified version of Equation~\ref{eq:methods:lfd-original} with $r_1 = 2 \cdot r_2$:
\begin{equation}
    \text{LFD}(q, r) = \text{log}_2 \left( \frac{|B(q, r)|}{|B(q, \frac{r}{2})|} \right).
    \label{eq:methods:lfd-half}
\end{equation}

% In other words, we assume that the dataset is embedded in a $D$-dimensional space, but that the data only occupy a $d$-dimensional manifold, where $d \ll D$.
% While we sometimes use Euclidean notions to describe the geometric and topological properties of the clusters and manifold, CLAM and CAKES do not rely on such notions;
% they serve merely as convenient and intuitive vocabulary to discuss the underlying mathematics.
% CAKES exploits the low LFD of such datasets to accelerate search.

Intuitively, LFD measures the rate of change in the number of points in a ball of radius $r$ around a point $q$ as $r$ increases.
When the vast majority of points in the dataset have low ($d \ll D$) LFD, we simply say that the dataset has low LFD.
We stress that this concept differs from the \textit{embedding dimension} of a dataset.
To illustrate the difference, consider the SILVA 18S rRNA dataset that contains genomic sequences with unaligned lengths of up to 3,712 bases and aligned length of 50,000 bases.
Hence, the \textit{embedding dimension} of this dataset is at least 3,712 and at most 50,000.
However, physical limitations (namely, biological evolution and biochemistry) constrain the data to a low-dimensional manifold within this space.
LFD is an approximation of the dimensionality of that low-dimensional manifold in the ``vicinity'' of a given point.
% Section~\ref{sec:results:lfd-of-datasets} discusses this concept on a variety of datasets, showing how real datasets uphold the manifold hypothesis.
For real-world datasets, we expect the LFD to be locally uniform, i.e.,\,when $r$ is small, but (potentially) globally variable.


\subsection{Clustering}
\label{sec:methods:clustering}

We define a \emph{cluster} $C$ as a set of points with a \emph{center} and \emph{radius}.
The center is the geometric median—the point minimizing the sum of distances to all other points—of the points in $C$;
in practice, for large $|C|$ we only compute this for a smaller sample.
A cluster's radius is the maximum distance from the center to any point in it.
Each non-leaf cluster has two children (as in a binary tree).
Clusters may have overlapping volumes, but each point in an overlap is assigned to exactly one cluster;
consequently $C(c,r)\subset B(c,r)$.
We denote the cluster tree by $\mathcal{T}$ and its root by $\mathcal{R}$ (Sec.~\ref{sec:methods:clustering:building-the-tree}).

We estimate the LFD of clusters at scales $r$ and $r/2$ via Eq.~\ref{eq:methods:lfd-half}, using only the points in $C(c,r)$ (not all of $B(c,r)$).

For a flat clustering, \emph{metric entropy} $\mathcal{N}_r(X)$ is the minimum number of radius-$r$ clusters needed to cover $X$~\cite{yu2015entropy}.
In our hierarchical setting, we define $\mathcal{N}_{\hat r}(X)$ as the number of leaf clusters, where $\hat r$ is the mean radius of all leaves.


\subsubsection{Building the Tree}
\label{sec:methods:clustering:building-the-tree}

We construct $\mathcal{T}$ using CLAM, following CHESS~\cite{ishaq2019clustered} but with improved pole selection, then apply a depth-first reordering (Sec.~\ref{sec:methods:clustering:depth-first-reordering}) on the dataset.
The procedure is reproduced in Alg.~\ref{alg:methods:partition} for completeness.

Starting from the root cluster $\mathcal{R}$ containing the entire dataset, Alg.~\ref{alg:methods:partition} recursively builds $\mathcal{T}$.
During Partition, we also compute and cache the LFD of each cluster using Eq.~\ref{eq:methods:lfd-half}.

\begin{algorithm} % enter the algorithm environment
    \caption{Partition($C$, $criteria$)} % give the algorithm a caption
    \label{alg:methods:partition} % and a label for \ref{} commands later in the document
    \begin{algorithmic} % enter the algorithmic environment
    \REQUIRE $f$, a distance function
    \REQUIRE $C$, a cluster
    \REQUIRE $criteria$, user-specified continuation criteria

    \STATE $S \Leftarrow$ random sample of $\left\lceil \sqrt{|C|} \right\rceil$ points from $C$
    \STATE $c \Leftarrow$ geometric median of $S$
    \STATE $l \Leftarrow \argmax f(c, x) \ \forall \ x \in C$
    \STATE $r \Leftarrow \argmax f(l, x) \ \forall \ x \in C$
    \STATE $L \Leftarrow \{x \ | \ x \in C \land f(l, x) \le f(r, x)\}$
    \STATE $R \Leftarrow \{x \ | \ x \in C \land f(r, x) < f(l, x)\}$

    \IF{$|L| > 1$ \textbf{and} $L$ satisfies $criteria$}
        \STATE Partition($L$, $criteria$)
    \ENDIF

    \IF{$|R| > 1$ \textbf{and} $R$ satisfies $criteria$}
        \STATE Partition($R$, $criteria$)
    \ENDIF
\end{algorithmic}
\end{algorithm}


\subsubsection{Depth-First Reordering}
\label{sec:methods:clustering:depth-first-reordering}

In CHESS~\cite{ishaq2019clustered}, each cluster stored a list of indices into the dataset.
This list was used to retrieve the clusters' points during search.
With a dataset of cardinality $n$ and each cluster storing a list of indices for its points, it stored $n$ indices at each depth in $\mathcal{T}$.
Assuming $\mathcal{T}$ is balanced, and thus $\mathcal{O}(\log n)$ depth, this approach had a prohibitively high memory overhead of $\mathcal{O}(n \log n)$.
In this work, we introduce a new approach wherein, after building $\mathcal{T}$, we reorder the dataset so that points are stored in a depth-first order.
Then, each cluster need only store its \textit{cardinality} and an \textit{offset} to access its points.
The root cluster $\mathcal{R}$ has an offset of zero and a cardinality equal to the number of points in the dataset.
A left child has the same offset its parent, and the corresponding right child has an offset equal to the left child's offset plus the left child's cardinality.
With no additional memory cost nor time cost for retrieving points during search, depth-first reordering offers the same time complexity as CHESS but with $\mathcal{O}(n)$ memory overhead.


\subsubsection{Time Complexity}
\label{sec:methods:clustering:time-complexity}

The asymptotic complexity of building $\mathcal{T}$ is the same as described in~\cite{ishaq2019clustered}, i.e., $\mathcal{O}(n \log n)$.
This is a significant improvement over the exact approach, which costs $\mathcal{O}(n^2 \log n)$.
The $\mathcal{O}(n\log n)$ build complexity assumes a balanced tree $\mathcal{T}$.
In practice, Alg.~\ref{alg:methods:partition} yields unbalanced trees on real data because of non-uniform sampling densities and the manifold's low-dimensional structure.
A balanced tree is expected only for uniformly distributed data (e.g., in a $d$-dimensional hypercube).

Although (unbalanced) trees are deeper than $\log n$, which may appear to increase the cost of tree-building, many leaves appear at shallow depths, so later levels contain fewer points and the per-level build cost declines.
Any analysis for unbalanced trees would be dataset-dependent, so we only report empirical results based on balanced version of Alg.~\ref{alg:methods:partition} in Section~\ref{sec:results:clustering-strategies-and-number-of-distance-computations}.
In any case, once the tree is built, its cost is amortized over all queries and subsequent applications.

\begin{figure}[t]
    \vskip -0.2in
    \centering
    \includegraphics[scale=0.5]{images/geometry/deltas.pdf}
    \vskip -0.1in
    \caption{
        {\color{blue}$\delta$}, {\color{red}$\delta^{+}$}, and {\color{green}$\delta^{-}$} for a cluster $C$ and a query $q$.
        ${\color{blue}\delta} = f(q, c)$ is the distance from the query to the cluster center $c$.
        ${\color{red}\delta^{+}} = \delta + r$ is the distance from the query to the theoretically farthest point in $C$.
        ${\color{green}\delta^{-}} = \text{max}(0, \delta - r)$ is the distance from the query to the theoretically closest point in $C$.
    }
    \label{fig:methods:deltas}
    % \vskip -0.5in
\end{figure}
\FloatBarrier

% \begin{minipage}{.425\textwidth}
\FloatBarrier
    \begin{algorithm}[!t]
        \caption{tree-search($C$, $q$, $r$)}
        \label{alg:methods:rnn-search:tree-search}
        \begin{algorithmic}[1]
            \REQUIRE $f$, a distance function
            \REQUIRE $C$, a cluster
            \REQUIRE $q$, a query
            \REQUIRE $r$, a search radius
            \IF{$\delta^+_C \leq r$}
                \STATE \textbf{return} $\{C\}$
            \ELSE
                \STATE $[L, R]$ $\Leftarrow$ \textit{children} of $C$
                \STATE \textbf{return} tree-search($L, q, r$) $\cup$ tree-search($R, q, r$)
            \ENDIF
        \end{algorithmic}
    \end{algorithm}
% \end{minipage}

% \begin{minipage}{.475\textwidth}
    \begin{algorithm}[!t]
        \caption{leaf-search($Q$, $q$, $r$)}
        \label{alg:methods:rnn-search:leaf-search}
        \begin{algorithmic}[1]
            \REQUIRE $f$, a distance function
            \REQUIRE $Q$, a set of clusters
            \REQUIRE $q$, a query
            \REQUIRE $r$, a search radius
            \STATE $H \Leftarrow \emptyset$
            \FOR{$C \in Q$}
                \IF{$\delta^+_C \leq r$}
                    \STATE $H$ $\Leftarrow$ $H \cup C$
                \ELSE
                    \FOR{$p \in C$}
                        \IF{$f(p, q) \leq r$}
                            \STATE $H$ $\Leftarrow$ $H \cup \{p\}$
                        \ENDIF
                    \ENDFOR
                \ENDIF
            \ENDFOR
            \STATE \textbf{return} $H$
        \end{algorithmic}
    \end{algorithm}
% \end{minipage}


\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{sec:methods:knn-search}

Given a user-specified integer $k$ and query $q$, $k$-NN search aims to find the $k$ closest points to $q$ in $\textbf{X}$.
In other words, $k$-NN search aims to find the set $H$ such that $|H| = k$ and $H = B(q, \rho_k)$ where $\rho_k = \max \left\{ f(q, p) \ \forall \ p \in H \right\}$ is the distance from $q$ to the $k^{th}$ nearest neighbor in $\textbf{X}$.

In this section, we present three novel algorithms for exact $k$-NN search:
Repeated $\rho$-NN, Breadth-First Sieve, and Depth-First Sieve.
In these algorithms, we use $H$ to refer to the data structure that stores the closest points to the query found so far, and $Q$ to refer to the data structure that stores the clusters and points that are still in contention for being among the $k$ nearest neighbors.
These algorithms also use some terminology defined and illustrated in Figure~\ref{fig:methods:deltas}.


\subsubsection{Repeated \texorpdfstring{$\rho$}{p}-NN}
\label{sec:methods:knn-search:repeated-rnn}

This algorithm relies on the \textit{tree-search} (Alg.~\ref{alg:methods:rnn-search:tree-search}) and \textit{leaf-search} (Alg.~\ref{alg:methods:rnn-search:leaf-search}) as described in~\cite{ishaq2019clustered} and reproduced here for completeness.

We begin by performing \textit{tree-search} with a search radius $r$ equal to the radius of the root $\mathcal{R}$ divided by $|X|$.
If no clusters are found, we double $r$ and repeat tree-search, until we find a a non-empty set, $Q$, of clusters.

Now, so long as $\sum_{C \in Q} |C| < k$, we continue to repeat tree-search, but instead of doubling $r$, we multiply it by a factor determined by the LFD in the vicinity of the query ball.
In particular, we increase the radius by a factor of
\begin{equation}
  \bigg( {\frac{k}{\sum_{C \in Q} |C|}} \bigg)^{\mu}
  \label{eq:methods:repeated-rnn-factor}
\end{equation}
where $\mu$ is the multiplicative inverse of the harmonic mean of the LFD of the clusters in $Q$, i.e.,\,$\mu = \frac{1}{|Q|} \cdot \sum_{C \in Q} \big( LFD(C)^{-1} \big)$.
We use the harmonic mean to ensure that $\mu$ is not dominated by outlier clusters with very high LFD.
In practice, we cap this factor at 2 to ensure that we do not increase the radius too quickly in any single iteration.

Intuitively, the factor by which we increase the radius should be \textit{inversely} related to the number of points found so far.
When the LFD at a given radius scale is high, the data are dense in that region.
Thus, a small increase in the radius would likely encounter many more points, and find $k$ neighbors.
Conversely, when the LFD at that scale is low, the data are sparse.
In such a region, a small increase in the radius would likely encounter vacant space, so a larger increase is needed.
Thus, the factor of radial increase should be \textit{inversely} related to the LFD.
However, we should not increase the radius too drastically with any one iteration because we assume that the LFD is only \textit{locally} uniform.
A large increase in the radius would likely break out of the local region and potentially encounter too many new clusters, which would make the subsequent step computationally expensive.

Once $\sum_{C \in Q} |C| \geq k$, we are guaranteed to have found at least $k$ neighbors, and so we perform \textit{leaf-search} to find the $k$ nearest neighbors among the points in the clusters found after the last tree-search.


\subsubsection{Complexity of Repeated \texorpdfstring{$\rho$}{p}-NN}
\label{sec:methods:knn-search:repeated-rnn-complexity}

% \begin{theorem} Let $X$ be a dataset and $q$ a query sampled from the same distribution (i.e., arising from the same generative process) as $X$. Then time complexity of performing Repeated $\rho$-NN search on $X$ with query $q$ is \begin{gather}
%         \mathcal{O}
%         \Bigg(
%             \underbrace{
%                 \log~\overbrace{\mathcal{N}_{\hat{r}}(X)}^{\textrm{metric entropy}}
%             }_{\textrm{tree-search}}
%             \ + \
%             \underbrace{
%                 \overbrace{k}^{\textrm{output size}} \cdot
%                 \overbrace{\bigg( 1 + 2 \cdot \Big( \frac{\hat{|C|}}{k} \Big) ^ {d^{-1}} \bigg)^d}^{\textrm{scaling factor}}
%             }_{\textrm{leaf-search}}
%         \Bigg)
%         \label{eq:methods:repeated-rnn-complexity}
%     \end{gather}
%     where $\mathcal{N}_{\hat{r}}(X)$ is the metric entropy of the dataset, $d$ is the LFD of the dataset, and $k$ is the number of nearest neighbors.
%     \label{thm:methods:rnn-complexity}
% \end{theorem}

We consider the \textit{tree-search} and \textit{leaf-search} stages of search separately.
Tree-search refers to the process of identifying clusters that overlap with the query ball, or in other words, clusters that might contain one of the $k$ nearest neighbors.
In ~\cite{ishaq2019clustered}, we showed that the complexity of $\rho$-NN search is
\begin{gather}
    \mathcal{O}
    \Bigg(
        \underbrace{
            \log~\overbrace{\mathcal{N}_{\hat{r}}(X)}^{\textrm{metric entropy}}
        }_{\textrm{tree-search}}
        +
        \underbrace{
            \overbrace{ \big| B(q, \rho) \big|}^{\textrm{output size}}
            \overbrace{ \left( \frac{\rho + 2 \cdot \hat{r}}{ \rho} \right) ^ {\text{LFD}(q, \rho)}}^{\textrm{scaling factor}}
        }_{\textrm{leaf-search}}
    \Bigg).
    \label{eq:methods:rnn-search-complexity}
\end{gather}

To extend Eq.~\ref{eq:methods:rnn-search-complexity} to Repeated $\rho$-NN, we must estimate the number of iterations of tree-search (Alg.~\ref{alg:methods:rnn-search:tree-search}) needed to find a radius that guarantees $k$ neighbors.
Since $q$ is sampled from the same distribution as $X$, the LFD near $q$ should not differ significantly from the LFDs of clusters near $q$ at the scale of the distance from the query to the $k^{th}$ nearest neighbor.
Thus, Equation~\ref{eq:methods:repeated-rnn-factor} suggests that in the expected case, we need only two iterations of tree-search to find $k$ neighbors:
one iteration to find at least one cluster, and one more to find enough clusters to guarantee $k$ neighbors.
Since this is a constant factor, complexity of tree-search for Repeated $\rho$-NN is the same as that for $\rho$-NN search, i.e.,\,$\mathcal{O}\big(\log\mathcal{N}_{\hat{r}}(X)\big)$.

We proceed to determine the complexity of leaf-search.
Let $Q$ be the set of clusters returned by tree-search.
We must estimate $\sum_{C \in Q} |C|$, the total cardinality of those clusters;
since we must examine every point in each such cluster, time complexity of leaf-search is linear in this quantity.
Let $\rho_k$ be the distance from the query to the $k^{th}$ nearest neighbor.
Then, we see that $Q$ is expected to be the set of clusters that overlap with a ball of radius $\rho_k$ around the query.
We can estimate this region as a ball of radius $\rho_k + 2\hat{r}$, where $\hat{r}$ is the mean radius of the clusters in $Q$.

The work in~\cite{yu2015entropy} showed that, for some constant $\gamma$,
\begin{equation*}
    \sum_{C \in S} |C| \leq \gamma \left| B(q, \rho_k) \right| \left(\frac{\rho_k + 2 \cdot \hat{r}}{\rho_k} \right)^d.
\end{equation*}

By definition of $\rho_k$, we have that $|B(q, \rho_k)| = k$.
Thus, $\sum_{C \in S} |C| \leq \gamma k \left( 1 + 2 \cdot \frac{\hat{r}}{\rho_k} \right)^d$.
It remains to estimate $\rho_k$.

Let $\hat{d}$ be the harmonic mean of the LFDs of the clusters in $Q$.
While ordinarily we compute LFD by comparing cardinalities of two balls with two different radii centered at \textit{the same} point, in order to estimate $\rho_k$, we instead compare the cardinality of a ball \textit{around the query} of radius $\rho_k$ to the mean cardinality, $\hat{|C|}$, of clusters in $Q$ at a radius equal to the mean of their radii, $\hat{r}$.
Since $q$ is from the same distribution as $X$, the LFD at $q$ should not be significantly different from that at the center of a cluster in $Q$.
By Equation~\ref{eq:methods:lfd-original}, we have $\hat{d} = \frac{\log{}\frac{\hat{|C|}}{k}}{\log{}\frac{\hat{r}}{\rho_k}}$, and rearranging, $\frac{\hat{r}}{\rho_k} = \left( \frac{\hat{|C|}}{k} \right)^{\hat{d}^{-1}}$.
Using this to simplify the term for leaf-search in Equation~\ref{eq:methods:rnn-search-complexity}, we get:
\begin{equation*}
    k \Bigg( 1 + 2 \cdot \bigg( \frac{\hat{|C|}}{k} \bigg) ^ {\hat{d}^{-1}} \Bigg)^d
\end{equation*}

Again using the assumption that $q$ is from the same distribution as $X$, the LFD at $q$ should not be significantly different from that at a cluster in $Q$, we have that $\hat{d} \approx d$.
By combining the bounds for tree-search and leaf-search, we see that time complexity of performing Repeated $\rho$-NN search on $X$ with query $q$ is
\begin{gather}
    \mathcal{O}
    \Bigg(
        \underbrace{
            \log~\overbrace{\mathcal{N}_{\hat{r}}(X)}^{\textrm{metric entropy}}
        }_{\textrm{tree-search}}
        +
        \underbrace{
            k \cdot
            \overbrace{\bigg( 1 + 2 \cdot \Big( \frac{\hat{|C|}}{k} \Big) ^ {d^{-1}} \bigg)^d}^{\textrm{scaling factor}}
        }_{\textrm{leaf-search}}
    \Bigg).
    \label{eq:methods:repeated-rnn-complexity}
\end{gather}

We remark that the scaling factor in Equation~\ref{eq:methods:repeated-rnn-complexity} should be close to 1 unless LFD is highly variable in the region around the query (i.e.,\,if $\hat{d}$ differs significantly from $d$).


\subsubsection{Breadth-First Sieve}
\label{sec:methods:knn-search:bredth-first-sieve}

This algorithm (BFS) performs a breadth-first traversal of $\mathcal{T}$, pruning clusters by using a modified version of the QuickSelect algorithm~\cite{hoare1961algorithm} (a subroutine in QuickSort), at each level of $\mathcal{T}$.

We begin by letting $Q$ be a list of 3-tuples $(p, \delta^{+}_{p}, m)$, where $p$ is either a cluster or a point, $\delta^{+}_{p}$ is the $\delta^{+}$ of $p$ as illustrated in Figure~\ref{fig:methods:deltas}, and $m$ is the multiplicity of $p$ in $Q$.
During the breadth-first traversal, for every cluster $C$ we encounter, we add $(C, \delta^{+}_{C}, |C| - 1)$ and $(c, \delta_{c}, 1)$ to $Q$, where $c$ is the center of $C$.
Recall that by the definitions of $\delta$ and $\delta^{+}$ given in Section~\ref{sec:methods:knn-search}, since $c$ is a point, $\delta_{C} = \delta_{c} = \delta^{+}_{c} = \delta^{-}_{c}$.

We then use the QuickSelect algorithm, modified to account for multiplicities and to reorder $Q$ in-place, to find the element in $Q$ with the $k^{th}$ smallest $\delta^{+}$; in other words, we find $\tau$, the smallest $\delta^{+}$ in $Q$ such that $\left| B(q, \tau) \right| \geq k$.
Since this step may require a binary search of $Q$ to find $\tau$ and reordering $Q$ takes linear time, this version of QuickSelect has $\mathcal{O}\left(|Q| \log |Q|\right)$ time complexity.

Next, we check every item in $Q$, removing any for which $\delta^{-} > \tau$ because they cannot contain (or be) one of the $k$ nearest neighbors.
If an item corresponds to a point, we keep it.
If it corresponds to a leaf cluster, we add all its points to $Q$ with a multiplicity of 1 each.
Otherwise (since it must correspond to a non-leaf cluster), we add to $Q$ the pairs of 3-tuples corresponding to its child clusters.
We continue this process until the sum of multiplicities in $Q$ is exactly $k$.
These are the $k$ nearest neighbors.

\subsubsection{Depth-First Sieve}
\label{sec:methods:knn-search:depth-first-sieve}

This algorithm (DFS) is similar to a depth-first traversal of $\mathcal{T}$.
It uses a max-priority queue to track hits, and a min-priority queue to choose the next branch of $\mathcal{T}$ to explore.

Let $Q$ be a min-queue of clusters prioritized by $\delta^{-}$ and $H$ be a max-queue (with capacity $k$) of points prioritized by $\delta$.
$Q$ starts containing only $\mathcal{R}$ while $H$ starts empty.
So long as $H$ is not full or the top-priority items from $H$ and $Q$ have $\delta_H \geq \delta_Q$, we repeat following:
\begin{itemize}
    \item While the top-priority cluster is not a leaf, remove it from $Q$ and add its children to $Q$.
    \item Remove the top-priority cluster (a leaf) from $Q$ and add all its points to $H$.
\end{itemize}
This process terminates when $H$ is full and the top-priority items in $H$ and $Q$ are such that $\delta_H < \delta_Q$, i.e.,\,the theoretically closest point left is farther from the query than the $k^{th}$ nearest neighbor found so far.
Thus, $H$ contains the $k$ nearest neighbors to the query.


\subsubsection{Complexity of Sieve Methods}
\label{sec:methods:knn-search:complexity-of-sieve-methods}

Due to their similarity, we combine the complexity analyses of both Sieve methods.
For these methods we again use the terminology of tree-search and leaf-search.
Tree-search navigates the cluster tree and finds $Q$.
Leaf-search exhaustively searches \textit{some} of the clusters in $Q$ to find the $k$ nearest neighbors.

% \begin{theorem}
%     Let $X$ be a dataset and $q$ a query sampled from the same distribution (i.e., arising from the same generative process) as $X$. Let $T \coloneqq \mathcal{O} \big( \lceil d \rceil \cdot \log \mathcal{N}_{\hat{r}}(X) \big)$ and   $L \coloneqq \mathcal{O} \left( k \cdot \bigg( 1 + 2 \cdot \Big( \frac{\hat{|C|}}{k} \Big) ^ {d^{-1}} \bigg)^d \right)$, where $\mathcal{N}_{\hat{r}}(X)$ is the metric entropy of $X$, $d$ is the LFD of $X$, $\hat{|C|}$ is the mean cardinality of clusters overlapping the query ball, and $k$ is the number of nearest neighbors. Then, for dataset $X$ and query $q$, the time complexity of performing Breadth-First Sieve search is \begin{equation}
%         \mathcal{O} \Big( (T + L ) \log (T + L ) \Big)
%         \label{eq:methods:breadth-first-sieve-complexity}
%     \end{equation} and the the time complexity of performing Depth-First Sieve search is \begin{equation}
%         \mathcal{O} \Big( T \log T + L \log k \Big).
%         \label{eq:methods:depth-first-sieve-complexity}
%     \end{equation}
%     \label{thm:methods:sieve-complexity}
% \end{theorem}

Since $q$ is sampled from the same distribution as $X$, the LFD near $q$ should not differ significantly from the LFDs of clusters near $q$ at the scale of the distance from the query to the $k^{th}$ nearest neighbor.
Let $d$ be the LFD in this region, and consider leaf clusters with cardinalities near $k$.
Then the number of leaf-clusters in $Q$ is bounded above by $2d$, where the bound is achieved by having a cluster overlap the query ball at each end of each of $\lceil d \rceil$ mutually-orthogonal axes.
In the worst-case scenario for tree-search, these leaf clusters would all come from different branches of the tree, and so tree-search would look at $2 \cdot \lceil d \rceil \cdot \log \mathcal{N}_{\hat{r}}(X)$ clusters.
Thus, the asymptotic complexity of tree-search is
\begin{equation*}
    T \coloneqq \mathcal{O} \big( \lceil d \rceil \cdot \log \mathcal{N}_{\hat{r}}(X) \big).
\end{equation*}
For leaf-search, the output size and scaling factor are the same as in Repeated $\rho$-NN, and so the complexity is
\begin{equation*}
    L \coloneqq \mathcal{O} \Big( k \cdot \Big( 1 + 2 \cdot \big( \frac{\hat{|C|}}{k} \big) ^ {d^{-1}} \Big)^d \Big).
\end{equation*}

The asymptotic complexity of BFS is dominated by QuickSelect.
Since QuickSelect is log-linear in the length of $Q$, and $Q$ contains the clusters from tree-search and the points from leaf-search, we see that the asymptotic complexity of BFS is
\begin{equation}
    \mathcal{O} \big( (T + L ) \log (T + L ) \big),
    \label{eq:methods:bfs-complexity}
\end{equation}
and that of DFS is
\begin{equation}
    \mathcal{O} \big( T \log T + L \log k \big).
    \label{eq:methods:dfs-complexity}
\end{equation}

\subsection{Auto-Tuning}
\label{sec:methods:auto-tuning}

We perform some simple auto-tuning to select the optimal $k$-NN algorithm to use with a given dataset.
We start by taking the center of every cluster at a low depth (e.g.,\,10) in $\mathcal{T}$ as a query.
This gives us a small, representative sample of the dataset.
Using these clusters' centers as queries, and a user-specified value of $k$, we record the time taken for $k$-NN search on the sample using each of the three algorithms described in Section~\ref{sec:methods:knn-search}.
We select the fastest algorithm over all the queries as the optimal algorithm for that dataset and value of $k$.
Note that even though we select the optimal algorithm based on use with some user-specified value of $k$, we still allow search with any value of $k$.


\subsection{Synthetic Data}
\label{sec:methods:synthetic-data}

Based on the asymptotic analysis, we expect CAKES to perform well on datasets with low LFD and scale sublinearly with cardinality.
To test this, we use datasets from the ANN-Benchmarks suite~\cite{aumuller2020ann} and synthetically augment them to have exponentially larger cardinalities.
We perform the same procedure on a dataset of uniformly distributed points in a hypercube, and compare CAKES against other algorithms on both the original and augmented datasets.

Our augmentation procedure is as follows.
Let $X$ be a dataset of dimension $d$, $\epsilon$ a user-specified noise level, and $m$ an integer multiplier.
For each $\mathbf{x}\in X$, generate $m-1$ new points within distance $\epsilon$ of $\mathbf{x}$ by sampling a random vector $\mathbf{r}\in\mathbb{R}^d$ from the $\epsilon$-ball (hypersphere of radius $\epsilon$ centered at the origin) and setting $\mathbf{x}'=\mathbf{x}+\mathbf{r}$.
Since $\lVert\mathbf{r}\rVert\le\epsilon$, we have $\lVert\mathbf{x}-\mathbf{x}'\rVert\le\epsilon$.
The result is $X'$ with $\lvert X'\rvert=m\,\lvert X\rvert$.
This increases cardinality by a factor of $m$ without altering the dataset’s overall topological structure, isolating the effect of cardinality from dimensionality, metric choice, and topology.
