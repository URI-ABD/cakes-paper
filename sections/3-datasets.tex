\section{Datasets And Benchmarks}
\label{sec:datasets-and-benchmarks}

% \subsection{Datasets}
% \label{sec:datasets-and-benchmarks:ann-benchmark-datasets}

Table~\ref{tab:datasets:summary} summarizes the cardinality and dimensionality of each dataset we use.
These include the Fashion-MNIST, Glove-25, and Sift datasets from the ANN-Benchmarks suite~\cite{aumuller2020ann}, as well as a Random dataset we generated (using the same cardinality and dimensionality as Sift) as an anti-example to demonstrate the effect of manifold structure on CAKES performance.
The Glove-25 dataset uses the Cosine distance function, while the other three datasets use the Euclidean distance metric.
All benchmarks using these datasets were conducted on an r6i.16xlarge instance on Amazon Web Services (AWS), with 64 vCPUs and 512 GiB of RAM.
The Rust compiler was Rust 1.83.0 and the Python interpreter version was 3.9.18.

We also include benchmarks on the SilVA-18S and Radio-ML datasets.
For the SilVA-18S dataset, which consists of 2,224,640 sequences of up-to 3,712 nucleotides, we use the Levenshtein distance metric~\cite{levenshtein1966binary}.
The Radio-ML dataset consists of 1,024-dimensional complex-valued time-series samples, spanning 24 modulation modes and 26 SNR levels from $-20$dB to $30$dB.
We use the 10dB SNR subset with 97,304 samples and Dynamic Time Warping~\cite{muller2007dynamic} (DTW) as the distance metric.
For both datasets, we withhold 1,000 random samples as queries and build the tree on the remaining samples.
These benchmarks were conducted on an Intel Xeon E5-2690 v4 CPU @ 2.60GHz with 512GB RAM running Manjaro Linux 5.15.164-1.

\begin{table}[h]
  \vskip -0.1in
  \caption{Datasets used in benchmarks.}
  \vskip -0.25in
  \label{tab:datasets:summary}
  \begin{center}
    \begin{sc}
      \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Dataset} & \textbf{Dist. Function}  &\textbf{Card}  & \textbf{Dim}  \\
        \hline
        Fashion-Mnist    & Euclidean                   & 60,000             & 784                    \\
        \hline
        Glove-25         & Cosine                      & 1,183,514          & 25                     \\
        \hline
        Sift             & Euclidean                   & 1,000,000          & 128                    \\
        \hline
        Random           & Euclidean                   & 1,000,000          & 128                    \\
        \hline
        SILVA            & Levenshtein                 & 2,224,640          & 3,712         \\
        \hline
        RadioML          & Dynamic Time Warping        & 97,920             & 1,024                  \\
        \hline
      \end{tabular}
    \end{sc}
  \end{center}
  \vskip -0.25in
\end{table}


\subsection{Scaling Experiments}
\label{sec:datasets-and-benchmarks:scaling-experiments}

In order to study how search performance scales as cardinality increases, we synthetically augment the ANN-Benchmarks datasets by generating new points by applying small random perturbations to existing points in the original dataset.
Tables~\ref{tab:results:qps-and-recall-fmn}, \ref{tab:results:qps-and-recall-glove}, \ref{tab:results:qps-and-recall-sift}, \ref{tab:results:qps-and-recall-random} show the results as the cardinality increases, with ``Mult.'' indicating the multiplier by which the cardinality was augmented.
Since the SilVA-18S and Radio-ML datasets are already large and are not vector datasets, we do not augment them;
instead, we subsample them to lower cardinalities for the scaling experiments.

\subsection{Local Fractal Dimension}
\label{sec:datasets:lfd-of-datasets}

Since the time complexity of CAKES algorithms scales with the LFD of the dataset, we examine, in Figure~\ref{fig:results:lfd-plots}, the LFD of each dataset we used for benchmarks.
\begin{figure}[h]
    \vskip -0.2in
    \centering
    % Row 1
    \subfloat[Fashion-MNIST\label{fig:results:fashion-mnist-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/fashion-mnist.png}}
    \hfill
    \subfloat[Glove-25\label{fig:results:glove-25-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/glove-25.png}}
    \vskip -0.2in

    % Row 2
    \par\medskip
    \subfloat[Sift\label{fig:results:sift-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/sift.png}}
    \hfill
    \subfloat[Random dataset\label{fig:results:random-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/random.png}}
    \vskip -0.2in

    % Row 3
    \par\medskip
    \subfloat[Silva 18S\label{fig:results:silva-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/silva-SSU-Ref.png}}
    \hfill
    \subfloat[RadioML\label{fig:results:radioml-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/radio-ml.png}}

    % Legend
    \par\smallskip
    \includegraphics[width=0.65\columnwidth]{images/lfd/legend.png}

    \caption{Local fractal dimension vs. cluster depth across six datasets; note the different y-axes. The x-axis is tree depth; the y-axis is LFD of clusters at that depth, computed using Equation~\ref{eq:methods:lfd-original} with $r_1 = r$ and $r_2 = \frac{r}{2}$ where $r$ is the radius of the cluster. We plot the 5th, 25th, 50th, 75th, and 95th percentiles, and min/max. The Fashion-MNIST, Glove-25, Sift and Silva datasets are real-world datasets and exhibit low LFD, indicating a manifold structure. The Random dataset has high LFD, indicating lack of manifold structure. The RadioML dataset is also a randomly generated dataset, but with a much more elaborate generation process. It seems to have little manifold structure, if any, instead exhibiting behavior akin to the Random dataset across three separate radial scales, as indicated by the three peaks in LFD at different depths in the tree.}
    \label{fig:results:lfd-plots}
\end{figure}

\subsection{Other Algorithms}
\label{sec:datasets-and-benchmarks:other-algorithms}

Tables~\ref{tab:results:qps-and-recall-fmn}, \ref{tab:results:qps-and-recall-glove}, \ref{tab:results:qps-and-recall-sift}, and \ref{tab:results:qps-and-recall-random} show the throughput (queries per second) and recall for each algorithm on each of the vector datasets.
Since HNSW, ANNOY, and FAISS do not support the Levenshtein or DTW distance functions, we only benchmark CAKES on the SilVA-18S and RadioML datasets and report results in Figures~\ref{fig:results:silva-scaling} and~\ref{fig:results:radioml-scaling}.
All benchmarks were run with $k=10$, throughput was averaged over all queries, and recall was computed using a na\"ive linear scan as ground truth.

HNSW, ANNOY, and FAISS have various hyper-parameters that can be tuned to trade off throughput, recall, and indexing time.
We allow for a hyper-parameter search (in the ranges recommended by the documentation accompanying the respective libraries) to find the configuration with the highest throughput that still achieves perfect recall (or as close to it as possible).
To make the comparison fair, we also allow CAKES to select the fastest of its algorithms by testing each with a small number of the centers of clusters at low depth in the tree as representative queries.
