\section{Datasets And Benchmarks}
\label{sec:datasets-and-benchmarks}

\subsection{ANN-Benchmark Datasets}
\label{sec:datasets-and-benchmarks:ann-benchmark-datasets}

We benchmark on a variety of datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
Table~\ref{tab:datasets:summary} summarizes the cardinality and dimensionality of each these datasets.
All benchmarks were conducted on an Intel Xeon E5-2690 v4 CPU @ 2.60GHz with 512GB RAM.
The OS kernel was Manjaro Linux 5.15.164-1-MANJARO.
The Rust compiler was Rust 1.83.0, and the Python interpreter version was 3.9.18.

\begin{table}
    \caption{Datasets used in benchmarks.}
    \label{tab:datasets:summary}
    \begin{center}
        \begin{sc}
            \begin{tabular}{|l|l|l|l|}
                \hline
                \textbf{Dataset} & \textbf{Dist. Function}  &\textbf{Card}  & \textbf{Dim}  \\
                \hline
                Fashion-Mnist    & Euclidean                   & 60,000             & 784                    \\
                \hline
                Glove-25         & Cosine                      & 1,183,514          & 25                     \\
                \hline
                Sift             & Euclidean                   & 1,000,000          & 128                    \\
                \hline
                Random           & Euclidean                   & 1,000,000          & 128                    \\
                \hline
                SILVA            & Levenshtein                 & 2,224,640          & 3,712         \\
                \hline
                RadioML          & Dynamic Time Warping        & 97,920             & 1,024                  \\
                \hline
            \end{tabular}
        \end{sc}
    \end{center}
    \vskip -0.1in
\end{table}


\subsection{Random Datasets and Synthetic Augmentation}
\label{sec:datasets-and-benchmarks:random-datasets}

In addition to the ANN-Benchmarks datasets, we evaluate synthetic augmentations generated as in Sec.~\ref{sec:methods:synthetic-data}, using $\epsilon=0.01$, and study scaling behavior as the multiplier (“Mult.” in Tables~\ref{tab:results:qps-and-recall-fmn}, \ref{tab:results:qps-and-recall-glove}, \ref{tab:results:qps-and-recall-sift}, \ref{tab:results:qps-and-recall-random}) increases.
We also benchmark a random dataset at various cardinalities;
we use a random dataset with base cardinality of $10^6$ and dimensionality $128$, and refer to this dataset as ``Random.''
This benchmark isolates the effect of manifold structure—expected to be absent in purely random data—on CAKES algorithms.


\subsection{SILVA 18S}
\label{sec:datasets-and-benchmarks:silva-18s}

To demonstrate CAKES with an exotic distance function, we use the SILVA 18S ribosomal RNA dataset~\cite{10.1093/nar/gks1219}, which contains rRNA sequences from 2,224,640 genomes;
the longest sequence being 3,712 letters.
Holding out 1,000 random sequences as queries, we build the tree and perform $k$-NN using Levenshtein distance~\cite{levenshtein1966binary} on \emph{unaligned} sequences.
Although this dataset is a multiple sequence alignment of width 50,000, we do not use Hamming distance on the aligned strings so as to demonstrate CAKES's flexibility.
Under Hamming, the embedding dimension would be 50,000;
under Levenshtein, the dimensionality is 3,712.

\subsection{Radio ML}
\label{sec:datasets-and-benchmarks:radio-ml}

We also use the Radio-ML dataset~\cite{oshea2018radioml} under Dynamic Time Warping~\cite{muller2007dynamic} as the metric.
This dataset contains 24 modulation modes spanning 26 SNR levels from $-20$dB to $30$dB, with 4,096 samples per combination, yielding 2,555,504 samples.
Each sample is a 1,024-dimensional complex-valued vector (a time-series).
For the benchmarks, we use the 10dB SNR subset with 97,304 samples and a 1,000-sample hold-out set of queries.

\subsection{Local Fractal Dimension}
\label{sec:dayasets:lfd-of-datasets}

Since the time complexity of CAKES algorithms scales with the LFD of the dataset, we examine the LFD of each dataset we used for benchmarks (Figure~\ref{fig:results:lfd-plots}).
In this section, when we discuss LFD, unless otherwise noted, we are referring to the 95th percentile from Figure~\ref{fig:results:lfd-plots}.

For Fashion-MNIST, Figure~\ref{fig:results:fashion-mnist-lfd}, we observe that until depth 5, LFD is less than 4. 
It then starts increasing, reaching a peak of about 6 near depth 20.
In comparison, Glove-25 has even lower LFD, as shown in Figure~\ref{fig:results:glove-25-lfd}. 
In particular, Glove-25's LFD is less than 3 for all depths.
Figure~\ref{fig:results:sift-lfd} shows the LFD for Sift, which is higher relative to both Fashion-MNIST and Glove-25, increasing sharply to a peak of 9 around depth 10.
For Random, which has the same cardinality and dimensionality as Sift, LFD starts at 20 the root cluster, and all percentile values decrease linearly with depth until reaching the leaves of the tree.
The spread in LFD starts narrow for the first few depths and widens as depth increases.
Silva, Figure~\ref{fig:results:silva-lfd}, exhibits consistently low LFD;
less than 3 for all depths, hovering near 1 for clusters at depth 40 and deeper.
For the Radio-ML dataset, Figure~\ref{fig:results:radioml-lfd} LFD values show three distinct peaks around an LFD of 12 near depths of 8, 25 and 50.
Each peak is followed by a linear decrease until encountering a sharp spike for the next peak.
Within each of the three spans of depths, the character of Radio-ML is similar to that of Random.

\begin{figure}[t]
    \centering
    % Row 1
    \subfloat[Fashion-MNIST\label{fig:results:fashion-mnist-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/fashion-mnist.png}}
    \hfill
    \subfloat[Glove-25\label{fig:results:glove-25-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/glove-25.png}}
  
    % Row 2
    \par\medskip
    \subfloat[Sift\label{fig:results:sift-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/sift.png}}
    \hfill
    \subfloat[Random dataset\label{fig:results:random-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/random.png}}
  
    % Row 3
    \par\medskip
    \subfloat[Silva 18S\label{fig:results:silva-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/silva-SSU-Ref.png}}
    \hfill
    \subfloat[RadioML\label{fig:results:radioml-lfd}]{
      \includegraphics[width=0.48\columnwidth]{images/lfd/radio-ml.png}}
  
    % Legend
    \par\smallskip
    \includegraphics[width=0.65\columnwidth]{images/lfd/legend.png}
  
    \caption{Local fractal dimension vs. cluster depth across six datasets. The ‘random’ dataset is generated as in Sec.~\ref{sec:datasets-and-benchmarks:random-datasets}; note the different y-axis. The x-axis is tree depth; the y-axis is LFD at that depth. We plot the 5th, 25th, 50th, 75th, and 95th percentiles, plus min/max. Each cluster is counted by its cardinality so the curves reflect dataset-level distributions.}
    \label{fig:results:lfd-plots}
  \end{figure}


% Since the CAKES algorithms were designed to scale in time with the LFD of the dataset, we examine the LFD of each dataset we used for benchmarks.
% Figure~\ref{fig:results:lfd-plots} illustrates the trends in LFD for Fashion-MNIST, Glove-25, Sift, Random, Silva 18S, and Radio-ML.
% In this section, when we discuss trends in LFD, unless otherwise noted, we are referring to the 95$^{th}$ percentile of LFD.

% For the Fashion-MNIST dataset we observe in Figure~\ref{fig:results:fashion-mnist-lfd} that until approximately depth 5, Fashion-MNIST's LFD is low (i.e., less than 4).
% It then starts increasing, reaching a peak of about 6 near depth 20, before decreasing to 1 at the maximum depth.

% Relative to Fashion-MNIST, Glove-25 has low LFD, as shown in Figure~\ref{fig:results:glove-25-lfd}.
% In particular, Glove-25's LFD is less than 3 for all depths.

% Figure~\ref{fig:results:sift-lfd} shows the LFD by depth for Sift, which has higher LFD relative to Fashion-MNIST and Glove-25.
% It increases sharply to a peak of 9 around a depth of 10, after which it decreases smoothly until reaching the deepest leaves in the tree.

% We contrast the low LFD of Sift with that of the Random dataset which has the same cardinality and dimension. As shown in Figure~\ref{fig:results:random-lfd}, the character of this dataset is very different from the others.
% The LFD starts at 20 at depth 0 and all percentile lines decrease linearly with depth until reaching the leaves of the tree.
% The spread in LFD starts very small for the first few clusters and increases as depth increases.

% The Silva dataset, shown in Figure~\ref{fig:results:silva-lfd}, exhibits consistently low LFD.
% In particular, LFD is less than 3 for all depths, hovering near 1 for clusters at depth 40 and deeper.

% For the Radio-ML dataset, the LFD values show three distinct peaks around an LFD of 12 at or near depths of 8, 25 and 50.
% Each peak is followed by a linear decrease until encountering a sharp spike for the next peak.
% Within each of the tree portions, this dataset has a character similar to that of the Random dataset.
% This suggests that the dataset obeys the manifold hypothesis at some scales, but that it is not ``scale free,'' as the LFD varies significantly by depth.
% This is likely the result of a piecewise uniform sampling strategy used to generate the different modulation modes present in the dataset.

\subsection{Other Algorithms}
\label{sec:datasets-and-benchmarks:other-algorithms}

We benchmark the three CAKES algorithms against a na\"ive linear search implementation in Rust.
We also benchmark against three state-of-the-art similarity search algorithms: HNSW, ANNOY, and FAISS-IVF in Python.
We verified that our linear search produces the same results as in the ground truth of the ANN-Benchmarks suite for Fashion-MNIST, Glove-25 and Sift datasets.
We then used this implementation to find and store the ground-truth for the augmented versions of those datasets, and use it to calculate recall for all algorithms.
We plot the results of these benchmarks in Figure~\ref{fig:results:scaling-plots}.
