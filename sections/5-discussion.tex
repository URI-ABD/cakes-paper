\section{Discussion and Future Work}
\label{sec:discussion-and-future-work}

We have presented CAKES, a suite of three algorithms for fast $k$-NN search across diverse distance functions.
When the distance is a metric (Sec.~\ref{sec:methods}), the algorithms are exact.
Under Cosine distance, a non-metric, they achieve nearly perfect recall.
CAKES is most effective when data satisfy the manifold hypothesis, i.e., they occupy a low-dimensional manifold within a high-dimensional space. 
It builds an unbalanced cluster tree that captures manifold structure and helps accelerate search. The results in Section~\ref{sec:results:clustering-strategies-and-number-of-distance-computations} shows that this unbalanced clustering approach is superior to balanced clustering in both absolute and asymptotic performance. 
Conversely, performance degrades on randomly distributed data, where such structure is absent.
In Figure~\ref{fig:results:lfd-plots}, we show the extent to which each dataset we tested exhibits a manifold structure, as quantified by the percentiles of LFD.  


\subsection{Performance}

As expected, Fig.~\ref{fig:results:scaling-plots} shows CAKES scaling sublinearly with cardinality on real-world datasets with low LFD and linearly on synthetic datasets with high LFD;
Sift versus Random illustrates this contrast.
On Random, CAKES' throughput decreases linearly with increasing cardinality, whereas on Sift, Depth-First Sieve and Breadth-First Sieve have near-constant throughput.
Repeated $\rho$-NN is not near-constant on Sift but decays much more slowly than on Random.
Because Sift and Random share cardinality and embedding dimension, this isolates the effect of manifold structure on performance.
Although CAKES is slower on Random, it maintains perfect recall.

HNSW and ANNOY maintain near-constant throughput as the cardinality grows, but their recalls decline and their index build time and memory rise, making them unsuitable when datasets grow exponentially.
CAKES's tree builds orders of magnitude faster with far less memory; while queries are slower than HNSW/ANNOY, CAKES attains near-constant scaling with perfect recall.

We stress that CAKES targets \emph{big} data, and our results support this claim.
We observe that while linear search can outperform CAKES on datasets with small cardinality, CAKES always overtakes linear search at a large enough cardinality for each of the ANN-benchmark datasets we tested.
Moreover, we observe that CAKES's algorithms outperform linear search at a lower cardinality when the dataset has a lower LFD.
For example, Sift has much higher LFD than Fashion-Mnist and Glove-25, and we observe that CAKES overtakes linear search at a cardinality of $10^{7}$  for Sift, as opposed to about $10^{5}$ and $10^{6}$ for Fashion-Mnist and Glove-25, respectively.
For the Random dataset, which has the highest LFD, CAKES's algorithms \emph{never} outperform linear search.

% When written with the same notation as used in Section~\ref{sec:methods:knn-search:complexity-of-sieve-methods}, we see that the time complexity of Repeated $\rho$-NN is $\mathcal{O}(\tfrac{T}{\lceil d \rceil} + L)$, where $T$ is the time complexity of tree-search and $L$ is the time complexity of leaf-search.
% Even though Repeated $\rho$-NN has the least time cost (compared to $\mathcal{O}\left((T + L)\log{(T+L)}\right)$ for Breadth-First Sieve and $\mathcal{O}(T\log{T} + L\log{k})$ for Depth-First Sieve) of the three CAKES algorithms, it is not always the fastest algorithm empirically.
% We believe that some of this discrepancy can be explained by the fact that Repeated $\rho$-NN can significantly ``overshoots'' the correct radius for $k$ hits ($\rho_k$) during tree-search.
% This causes leaf-search to exhaustively search many more clusters than necessary, rendering the true scaling factor higher than that in Equation~\ref{eq:methods:repeated-rnn-complexity}.
% This overshot can occur when the LFDs of clusters near the query are not concentrated around their expectation.
% For example, if most clusters near the query have very low LFD except for one anomaly with very high LFD,
% the harmonic mean LFD $\mu$ can still be low, so the factor of radial increase in (Equation)~\ref{eq:methods:repeated-rnn-factor} may be much larger than necessary for guaranteeing $k$ hits.
% This suggests that rather than using the reciprocal of the harmonic mean LFD in~\ref{eq:methods:repeated-rnn-factor}, we may achieve better results with a mean that is more sensitive to high outliers, such as the geometric mean.
% We leave it as an avenue for future work to characterize when Repeated $\rho$-NN significantly overestimates the correct radius $\rho_k$ and to improve upon the factor of radial increase in Equation~\ref{eq:methods:repeated-rnn-factor} so that this occurs less frequently and with less severity.


\subsection{Future Work}

This study can be extended in several directions.
We can start by evaluating more datasets and distance functions not supported by FAISS, HNSW, or ANNOY, such as Wasserstein~\cite{vallender1974calculation} for probability distributions (particularly high-dimensional) and Tanimoto~\cite{bajusz2015tanimoto} for molecular structures.

We can also improve the augmentation procedure (Sec.~\ref{sec:methods:synthetic-data}) to better preserve topological structure of datasets, e.g., by favoring perturbations along the leading principal components of the local manifold.
This would better test the scaling behavior of various algorithms.

We have noted how the fastest algorithm in CAKES varies depending on the dataset and distance function being used, as well as the cardinality of the dataset.
This variability motivated our auto-tuning process (Sec.~\ref{sec:methods:auto-tuning}).
Future work should examine why and when each algorithm excels and learn to use the geometric and topological properties of datasets to choose the ideal algorithm to use.

CAKES algorithms can also be extended for use in a streaming environment, enabling online updates to the tree as points are inserted or deleted.

Despite CAKES' generality, effective deployment on real data requires domain knowledge.
In protein sequence search, for example, sequences vary widely in length (typically 30-1000 amino acids, with outliers) and have a 20-letter alphabet.
Numerous algorithms exist~\cite{kim2021entrance, daniels2013compressive, yu2015entropy, steinegger2018clustering}, and their development relied on deep domain expertise.
Simple edit distance can be poor when lengths vary substantially.
CAKES can get around that issue by instead using alternative metrics or preprocessingâ€”for example, represent each sequence as a set of $k$-mers and use Jaccard distance, as in~\cite{kim2021entrance}.
If the distance relies on global alignment (e.g., Needleman-Wunsch~\cite{needleman1970general}), the dataset can be partitioned into mutually exclusive length bins with a separate tree per bin;
at query time, we simply route the sequence to its bin by length, alogn with a small number of adjacent bins.

\subsection{Availability}

CLAM and CAKES are implemented in Rust and available under the MIT license at https://github.com/URI-ABD/clam.
