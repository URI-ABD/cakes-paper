\section{Discussion and Future Work}
\label{sec:discussion-and-future-work}

We have presented CAKES, a suite of three algorithms for fast $k$-NN search across diverse distance functions. When the distance is a metric (Sec.~\ref{sec:methods}), the algorithms are exact; under cosine distance (non-metric) they achieve nearly perfect recall. CAKES is most effective when data satisfy the manifold hypothesis—lying on a low-dimensional manifold within a high-dimensional space. It builds an unbalanced-clustering binary tree that captures manifold structure and accelerates search. Conversely, performance degrades on randomly distributed data, where such structure is absent.


\subsection{Performance}

As expected, Fig.~\ref{fig:results:scaling-plots} shows CAKES scaling sublinearly with cardinality on real-world datasets with low LFD and linearly on synthetic datasets with high LFD; Sift versus Random illustrates this contrast.

On Random, CAKES throughput decreases linearly with the multiplier, whereas on Sift, Depth-First Sieve and Breadth-First Sieve are nearly constant. Repeated $\rho$-NN is not near-constant on Sift but decays much more slowly than on Random. Because Sift and Random share cardinality and embedding dimension, this isolates the effect of manifold structure on performance.

Although CAKES is slower on Random, its recall remains perfect. HNSW and ANNOY maintain near-constant throughput as the multiplier grows, but their recall declines, and their index build time and memory rise, making them unsuitable when datasets grow exponentially. CAKES’s tree builds orders of magnitude faster with far less memory; while queries are slower than HNSW/ANNOY, CAKES attains near-constant scaling with perfect recall.

The fastest CAKES variant depends on dataset and cardinality. On Fashion-MNIST, Glove-25, and Sift, Depth-First Sieve and Breadth-First Sieve show nearly constant throughput as cardinality increases and consistently outperform linear search at high cardinalities. This variability motivates auto-tuning (Sec.~\ref{sec:methods:auto-tuning}); future work will better characterize when each algorithm excels and enable more sophisticated selection from dataset properties.

We stress that CAKES targets \emph{big} data: linear search is better at small sizes, but for each ANN-Benchmarks dataset tested, CAKES overtakes linear search at sufficiently large cardinality—about $10^{5}$ for Fashion-MNIST and $10^{6}$ for Glove-25. On the high-LFD Random dataset, CAKES never beats linear search. These results support that CAKES performs well on data arising from constrained generating phenomena even as cardinality grows exponentially.



% When written with the same notation as used in Section~\ref{sec:methods:knn-search:complexity-of-sieve-methods}, we see that the time complexity of Repeated $\rho$-NN is $\mathcal{O}(\tfrac{T}{\lceil d \rceil} + L)$, where $T$ is the time complexity of tree-search and $L$ is the time complexity of leaf-search.
% Even though Repeated $\rho$-NN has the least time cost (compared to $\mathcal{O}\left((T + L)\log{(T+L)}\right)$ for Breadth-First Sieve and $\mathcal{O}(T\log{T} + L\log{k})$ for Depth-First Sieve) of the three CAKES algorithms, it is not always the fastest algorithm empirically.
% We believe that some of this discrepancy can be explained by the fact that Repeated $\rho$-NN can significantly ``overshoots'' the correct radius for $k$ hits ($\rho_k$) during tree-search.
% This causes leaf-search to exhaustively search many more clusters than necessary, rendering the true scaling factor higher than that in Equation~\ref{eq:methods:repeated-rnn-complexity}.
% This overshot can occur when the LFDs of clusters near the query are not concentrated around their expectation.
% For example, if most clusters near the query have very low LFD except for one anomaly with very high LFD,
% the harmonic mean LFD $\mu$ can still be low, so the factor of radial increase in (Equation)~\ref{eq:methods:repeated-rnn-factor} may be much larger than necessary for guaranteeing $k$ hits.
% This suggests that rather than using the reciprocal of the harmonic mean LFD in~\ref{eq:methods:repeated-rnn-factor}, we may achieve better results with a mean that is more sensitive to high outliers, such as the geometric mean.
% We leave it as an avenue for future work to characterize when Repeated $\rho$-NN significantly overestimates the correct radius $\rho_k$ and to improve upon the factor of radial increase in Equation~\ref{eq:methods:repeated-rnn-factor} so that this occurs less frequently and with less severity.


\subsection{Applicability}

Despite CAKES’s generality, effective deployment on real data can require domain knowledge. In protein sequence search, sequences vary widely in length (typically 30–1000 amino acids, with outliers) over a 20-letter alphabet. Numerous algorithms exist~\cite{kim2021entrance, daniels2013compressive, yu2015entropy, steinegger2018clustering}, and their development relied on deep domain expertise.

Simple edit distance can be poor when lengths vary substantially. CAKES can get around that issue by instead using alternative metrics or preprocessing—for example, represent each sequence as a set of $k$-mers and use Jaccard distance, as in~\cite{kim2021entrance}. If the distance relies on global alignment (e.g., Needleman–Wunsch~\cite{needleman1970general}), the dataset can be partitioned into mutually exclusive length bins with a separate CAKES tree per bin; at query time, route the sequence to its bin and a small number of adjacent bins.


\subsection{Future Work}

This study suggests several directions. First, we want to extended our evaluation to more datasets and distance functions not supported by FAISS, HNSW, or ANNOY, including Wasserstein distance~\cite{vallender1974calculation} for probability distributions (particularly high-dimensional) and Tanimoto distance~\cite{bajusz2015tanimoto} for comparing molecular structures via maximal common subgraphs. Adding a new distance to CAKES requires only a Rust implementation.

Second, we intend to improve the augmentation procedure (Sec.~\ref{sec:methods:synthetic-data}) to better preserve topological structure of datasets, e.g., by favoring perturbations along the leading principal components of the local manifold.

Third, we plan to explore CAKES algorithms' use in a streaming environment, enabling online updates to the tree as points are inserted or deleted. 

Finally, extend anomaly detection in CHAODA~\cite{ishaq2021clustered} by adding graph-based methods that use the distribution of distances among the $k$ nearest neighbors of cluster centers.

\subsection{Availability}

CLAM and CAKES are implemented in Rust and released under the MIT license at https://github.com/URI-ABD/clam.
