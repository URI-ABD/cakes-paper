\section{Introduction}
\label{sec:introduction}

Researchers are collecting data at an unprecedented rate, with datasets in many fields growing exponentially, and outpacing improvements in computing performance predicted by Moore's Law~\cite{kahn2011future}.
Examples include genomic databases~\cite{10.1093/nar/gks1219}, time-series signals~\cite{oshea2018radioml}, and neural embeddings~\cite{2020arXiv200514165B, OpenAI2023GPT4TR, Touvron2023Llama2O, radford2021learning, dosovitskiy2020image}.
This ``Big Data explosion'' has created a need for algorithms that scale efficiently to large datasets.
One such class of algorithms is similarity search;
yet as datasets grow, fast and accurate similarity search becomes increasingly challenging.
Even state-of-the-art algorithms exhibit a steep tradeoff between recall and throughput~\cite{malkov2016hnsw, johnson2019billion, annoy, aumuller2020ann}.

Given a distance function on a dataset, $k$-nearest neighbors ($k$-NN) search finds the $k$ closest points to a query.
We call algorithms with imperfect recall \textit{approximate}, and those matching brute-force search \textit{exact}.
Most fast methods are approximate~\cite{gao2023high}, and while they are sufficient for some applications, there remains a need for \textit{fast exact} search~\cite{ukey2023survey}.

This paper introduces CAKES (CLAM-Accelerated $K$-NN Entropy-Scaling Search), a set of three novel algorithms for exact $k$-NN search.
We benchmark CAKES against FAISS~\cite{johnson2019billion}, HNSW~\cite{malkov2016hnsw}, and ANNOY~\cite{annoy}, on datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
We also evaluate performance on SILVA 18S~\cite{10.1093/nar/gks1219} using Levenshtein~\cite{levenshtein1966binary} distance on unaligned genomic sequences, and RadioML~\cite{oshea2018radioml} using Dynamic Time Warping (DTW)~\cite{gold2018dynamic} distance on complex-valued time-series.
To further contextualize results, we also use a synthetic dataset with simple statistical properties.


\subsection{Related Works}
\label{sec:intoduction:related-works}

Modern k-NN search strategies include entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered} which leverages manifold structure, Hierarchical Navigable Small World networks (HNSW)~\cite{malkov2016hnsw} which relies on NSW networks and skip lists, InVerted File indexing (FAISS-IVF)~\cite{faissivf} which creates high-dimensional Voronoi cells, and ANNOY~\cite{annoy}, which builds several trees under different random projections.
Crucially, many of these algorithms do not support \emph{exact} search.


\subsection{Entropy-Scaling Search}
\label{sec:intoduction:entropy-scaling-search}

Entropy-scaling algorithms exploit the inherent manifold structure of large datasets, achieving performance that scales with topological properties, such as metric entropy and fractal dimension, rather than cardinality.
This makes them especially suitable for manifold-constrained data.

CAKES is a set of three such algorithms for exact $k$-NN search: Repeated $\rho$-NN, Breadth-First Sieve, and Depth-First Sieve.
In the next section, we describe the clustering and search algorithms, and provide a complexity analysis.
Our analysis assumes datasets with manifold structure, reflecting the conditions for which CAKES is designed.
We expect, and show in Section~\ref{sec:results}, that CAKES performs well on such datasets, and poorly when manifold structure is absent.
