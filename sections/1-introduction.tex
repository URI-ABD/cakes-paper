\section{Introduction}
\label{sec:introduction}

Researchers are collecting data at an unprecedented rate, with datasets in many fields growing exponentially, and outpacing improvements in computing performance predicted by Moore's Law~\cite{kahn2011future}.
Examples include genomic databases~\cite{10.1093/nar/gks1219}, time-series signals~\cite{oshea2018radioml}, and neural embeddings~\cite{2020arXiv200514165B, OpenAI2023GPT4TR, Touvron2023Llama2O, radford2021learning, dosovitskiy2020image}.
This ``Big Data explosion'' has created a need for algorithms that scale efficiently to large datasets.
One such class of algorithms is similarity search;
yet as datasets grow, fast and accurate similarity search becomes increasingly challenging.
Even state-of-the-art algorithms exhibit a steep tradeoff between recall and throughput~\cite{malkov2016hnsw, johnson2019billion, annoy, aumuller2020ann}.

Given a distance function for the data, $k$-nearest neighbors ($k$-NN) search finds the $k$ points in a dataset that are closest to a given query point.
We use the term \textit{approximate} to refer to algorithms that have imperfect recall, and \textit{exact} to refer to algorithms that have perfect recall as measured against a brute-force search.
Existing fast algorithms are often approximate~\cite{gao2023high}, which may suffice for some applications, but the need for \textit{fast and exact} search remains~\cite{ukey2023survey}.

This paper introduces CAKES (CLAM-Accelerated $K$-NN Entropy-Scaling Search), a set of three novel algorithms for exact $k$-NN search.
We benchmark CAKES against FAISS~\cite{johnson2019billion}, HNSW~\cite{malkov2016hnsw}, and ANNOY~\cite{annoy}, on datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
We also evaluate performance on SILVA 18S~\cite{10.1093/nar/gks1219} using Levenshtein~\cite{levenshtein1966binary} distance on unaligned genomic sequences, and RadioML~\cite{oshea2018radioml} using Dynamic Time Warping (DTW)~\cite{gold2018dynamic} distance on complex-valued time-series.
To further contextualize results, we also use a synthetic dataset with simple statistical properties.


\subsection{Related Works}
\label{sec:intoduction:related-works}

Modern k-NN search strategies include entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}, Hierarchical Navigable Small World networks (HNSW)~\cite{malkov2016hnsw}, InVerted File indexing (FAISS-IVF)~\cite{faissivf}, and random projection and tree building (ANNOY)~\cite{annoy}.
HNSW~\cite{malkov2016hnsw} relies on navigable small world (NSW) networks and skip lists.
FAISS-IVF~\cite{faissivf, sacks1987multikey, kent1990signature} partitions data into high-dimensional Voronoi cells, and searches only within cells near the query.
Crucially, many of these algorithms do not support \emph{exact} search.


\subsection{Entropy-Scaling Search}
\label{sec:intoduction:entropy-scaling-search}

Entropy-scaling algorithms exploit the inherent manifold structure of large datasets, achieving performance that scales with topological properties, such as metric entropy and fractal dimension, rather than cardinality.
This makes them especially suitable for manifold-constrained data.

CAKES is a set of three such algorithms for exact $k$-NN search: Repeated $\rho$-NN, Breadth-First Sieve, and Depth-First Sieve.
In the next section, we describe the clustering and search algorithms, and provide a complexity analysis.
These analyses are not worst-case analyses in the traditional sense, as they do not assume the worst possible dataset, namely, a random uniform distribution.
Since CAKES is designed for datasets with a manifold structure, an analysis assuming a uniform distribution would be uninformative.
Therefore, our analysis assumes datasets with manifold structure, reflecting the conditions for which CAKES is designed.
