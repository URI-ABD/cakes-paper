% SIAM Supplemental File Template
\newif\ifarxiv
\arxivtrue % swap this to true to build for the arxiv
\ifarxiv
\documentclass{article}
\usepackage{arxiv}
\else
\documentclass[review,supplement,onefignum,onetabnum]{siamonline220329}
\fi

\input{cakes_shared}
\begin{document}
% Optional PDF information
\ifarxiv
\else
\ifpdf
\hypersetup{
  pdftitle={Supplementary Materials: Let them have CAKES: A Cutting-Edge Algorithm for Scalable, Efficient, and Exact Search on Big DataAn Example Article},
  pdfauthor={M. E. Prior, T. J. Howard III, O. McLaughlin, T. Ferguson, N. Ishaq, N. M. Daniels}
}
\fi
\fi

\maketitle


\section{Supplementary Methods}

\subsection{Algorithm Details}

\begin{algorithm} % enter the algorithm environment
    \caption{Partition($C$, $criteria$)} % give the algorithm a caption
    \label{alg:methods:partition} % and a label for \ref{} commands later in the document
    \begin{algorithmic} % enter the algorithmic environment
    \Require $f: X \times X \mapsto \mathbb{R}^+ \cup \{0\}$, a distance function
    \Require $C$, a cluster
    \Require $criteria$, user-specified continuation criteria

    \State $seeds \Leftarrow$ random sample of $\left\lceil \sqrt{|C|} \right\rceil$ points from $C$
    \State $c \Leftarrow$ geometric median of $seeds$
    \State $l \Leftarrow \argmax f(c, x) \ \forall \ x \in C$
    \State $r \Leftarrow \argmax f(l, x) \ \forall \ x \in C$
    \State $L \Leftarrow \{x \ | \ x \in C \land f(l, x) \le f(r, x)\}$
    \State $R \Leftarrow \{x \ | \ x \in C \land f(r, x) < f(l, x)\}$

    \If{$|L| > 1$ \textbf{and} $L$ satisfies $criteria$}
        \State Partition($L$, $criteria$)
    \EndIf

    \If{$|R| > 1$ \textbf{and} $R$ satisfies $criteria$}
        \State Partition($R$, $criteria$)
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm} % enter the algorithm environment
    \caption{Repeated $\rho$-NN($\mathcal{R}$, $q$, $k$)} % give the algorithm a caption
    \label{alg:methods:repeated-rnn} % and a label for \ref{} commands later in the document
    \begin{algorithmic} % enter the algorithmic environment
        \Require $\mathcal{R}$, the root cluster
        \Require $q$, a query
        \Require $k$, the number of neighbors to find
        \State{$H \Leftarrow [\ ]$, a max-heap by $\delta$ of size $k$}
        \State $r \Leftarrow radius$ of $\mathcal{R}$
        \State $r \Leftarrow$ $\frac{r}{|\mathcal{R}|}$
        \State $Q \Leftarrow$ tree-search($\mathcal{R}$, $q$, $r$)
        \While{$\sum_{C \in Q} |C| < k$}
            \If{$Q = \emptyset$}
                \State $r \Leftarrow 2 \cdot r$
            \ElsIf{$\sum_{C \in Q} |C| >= k$}
                \State $\mu \Leftarrow \frac{1}{|Q|} \cdot \sum_{C \in Q} \big( LFD(C)^{-1} \big)$
                \State $r \Leftarrow r \cdot \min \bigg( 2, \left( {\frac{k}{\sum_{C \in Q} |C|}} \right)^{\mu} \bigg)$
            \EndIf
            \State $Q \Leftarrow$ tree-search($\mathcal{R}$, $q$, $r$)
        \EndWhile
        \State $H \Leftarrow \text{leaf-search}(Q, q, r)$
        \State \textbf{return} $H$ as a list
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Breadth-First Sieve($\mathcal{R}$, $q$, $k$)} % give the algorithm a caption
    \label{alg:methods:bredth-first-sieve} % and a label for \ref{} commands later in the document
    \begin{algorithmic} % enter the algorithmic environment
        \Require $\mathcal{R}$, the root cluster
        \Require $q$, a query
        \Require $k$, the number of neighbors to find
        \State $c \Leftarrow$ \textit{center} of $\mathcal{R}$
        \State $Q \Leftarrow$ \{ ($\mathcal{R}$, $\delta^{+}_{\mathcal{R}}$, $|\mathcal{R}| - 1$), ($c$, $\delta_{\mathcal{R}}$, 1) \}
        \While{$\sum_{(\_, \_, m) \in Q} m \neq k$}
            \State $\tau \Leftarrow$ QuickSelect($Q$, $k$)
            \State $Q^{'} \Leftarrow \emptyset$
            \For{$(C, \_, \_) \in Q$}
                \If{$\delta^{-}_{C} \leq \tau$}
                    \If{$C$ is a point}
                        \State $Q^{'} \Leftarrow Q^{'} \cup \{ (C, \delta_{C}, 1) \}$
                    \ElsIf{$C$ is a leaf}
                        \State $Q^{'} \Leftarrow Q^{'} \cup \{ (p, \delta_{p}, 1)$ for $p \in C \}$
                    \Else
                        \State $[L, R] \Leftarrow$ children of $C$
                        \State $l, r \Leftarrow$ centers of $L, R$
                        \State $Q^{'} \Leftarrow Q^{'} \cup \{ (L, \delta^{+}_{L}, |L| - 1), (l, \delta_{L}, 1) \}$
                        \State $Q^{'} \Leftarrow Q^{'} \cup \{ (R, \delta^{+}_{R}, |R| - 1), (l, \delta_{R}, 1) \}$
                    \EndIf
                \EndIf
            \EndFor
            \State $Q \Leftarrow Q^{'}$
        \EndWhile
        \State QuickSelect($Q$, $k$)
        \State \textbf{return} The first $k$ points in $Q$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Depth-First Sieve($\mathcal{R}$, $q$, $k$)}
    \label{alg:methods:depth-first-sieve}
    \begin{algorithmic}
        \Require $\mathcal{R}$, the root cluster
        \Require $q$, a query
        \Require $k$, the number of neighbors to find
        \State{$Q \Leftarrow [\mathcal{R}]$, a min-heap by $\delta^{-}$}
        \State{$H \Leftarrow [\ ]$, a max-heap by $\delta$ of size $k$}
        \While{$|H| < k$ \textbf{or} $H.peek.\delta \geq Q.peek.\delta^{-}$}
            \While{$Q.peek$ is not a leaf}
                \State{$C \Leftarrow Q.pop$, the closest cluster}
                \State{$[L, R] \Leftarrow$ children of $C$}
                \State{$Q.push(L)$}
                \State{$Q.push(R)$}
            \EndWhile
            \State{$leaf \Leftarrow Q.pop$}
            \For{$p \in leaf$}
                \State{$H.push(p)$}
            \EndFor
        \EndWhile
        \State \textbf{return} $H$
    \end{algorithmic}
\end{algorithm}


\subsection{Algorithm Details}
\label{sec:supplement:algorithm-details}

\subsection{\texorpdfstring{$\rho$}{p}-Nearest Neighbors Search}

We conduct $\rho$-NN search as described in~\cite{ishaq2019clustered}, but with the following improvement:
when a cluster overlaps with the query ball, instead of always proceeding to search both of its children, we proceed only with those children that might contain points in the query ball.

To determine whether both children can contain points in the query ball, we consider Figure~\ref{fig:supplement:overlapping-children}.
Here, we overload the notation for $\overline{x y}$ to refer both to the line segment joining points $x$ and $y$ as well as to the length of that line segment.

Let $q$ denote the query, $\rho$ denote the search radius, and $l$ and $r$ denote the cluster's left and right poles respectively.
Without loss of generality, we assume that $\overline{q r \vphantom{l}} \leq \overline{q l}$.
Now let $q'$ be the projection of $q$ onto $\overline{l r}$, $m$ be the midpoint of $\overline{l r}$, and $d$ be the distance from $q'$ to $m$.
As a consequence of how we assign a point in the parent cluster to the left child in the Partition algorithm, if $\rho < d$, then the left child cannot contain points inside the query ball.
In such a case we proceed to search only the right child.
Otherwise, we proceed with both children.

To check whether $d \leq \rho$, we note that $d = \overline{m q' \vphantom{l}} = \overline{m r \vphantom{l}} - \overline{q' r \vphantom{l}} = \frac{\overline{l r}}{2} - \overline{q' r \vphantom{l}}$.
Let $\theta$ denote $\angle l r q$, as shown in Figure~\ref{fig:supplement:overlapping-children}.
By the Law of Cosines on $\triangle l r q$, we have that $\text{cos}(\theta) = \tfrac{\overline{l r}^2 + \ \overline{q r \vphantom{l}}^2 - \ \overline{q l}^2}{2 \cdot \overline{l r} \cdot \overline{q r \vphantom{l}}}$.
Since $\triangle r q q'$ is a right triangle, we also have that $\text{cos}(\theta) = \tfrac{\overline{q' r \vphantom{l}}}{\overline{q r \vphantom{l}}}$.
Combining the previous two equations and solving for $\overline{q' r \vphantom{l}}$, we have that $\overline{q' r \vphantom{l}} = \tfrac{\overline{q r \vphantom{l}}^2 + \ \overline{l r}^2 - \ \overline{q l}^2}{2 \cdot \overline{l r}}$.
Substituting for $\overline{q' r \vphantom{l}}$ in the equation for $d$, we have that $d = \tfrac{\overline{l r}}{2} - \tfrac{\overline{q r \vphantom{l}}^2 + \ \overline{l r}^2 - \ \overline{q l}^2}{2 \cdot \overline{l r}} = \tfrac{\overline{q l}^2 - \overline{q r \vphantom{l}}^2}{2 \cdot \overline{l r}}$.


Thus, $d \leq \rho \iff (\overline{q l} + \overline{q r \vphantom{l}})(\overline{q l} - \overline{q r \vphantom{l}}) \leq 2 \cdot \overline{l r} \cdot \rho$. Note, in particular, that this only requires distances between actual points from the dataset, and so it can be used with any distance function, even when $q'$ and $m$ are not actual points or cannot be imputed from the data.

To perform $\rho$-NN search, we first perform a coarse \textit{tree-search}, to find the leaf clusters that overlap with the query ball or any clusters which lie entirely within the query ball.
Then, for all such clusters, we perform a finer-grained \textit{leaf-search}, to find all points that are no more than a distance $\rho$ from the query.
The asymptotic complexity of $\rho$-NN is the same as in~\cite{ishaq2019clustered} and shown in Equation~\ref{eq:supplement:rnn-search-complexity}.

\begin{gather}
    \mathcal{O}
    \Bigg(
        \underbrace{
            \log~\overbrace{\mathcal{N}_{\hat{r}}(X)}^{\textrm{metric entropy}}
        }_{\textrm{tree-search}}
        \ + \
        \underbrace{
            \overbrace{ \big| B_X(q, \rho) \big|}^{\textrm{output size}}
            \overbrace{ \left( \frac{\rho + 2 \cdot \hat{r}}{ \rho} \right) ^ d}^{\textrm{scaling factor}}
        }_{\textrm{leaf-search}}
    \Bigg)
    \label{eq:supplement:rnn-search-complexity}
\end{gather}
where $\hat{r}$ is the \textit{mean} radius of leaf clusters, $\mathcal{N}_{\hat{r}}(X)$ is the metric entropy at that radius, $B_X(q, \rho)$ is a ball of radius $\rho$ around the query $q$, and $d$ is the LFD around the query at the length scale of $\rho$ and $\rho + 2 \cdot \hat{r}$.


\begin{figure}
    \centering
    \includegraphics[scale=0.75]{images/geometry/overlapping-children-3.pdf}
    \caption{The geometry of a query ball overlapping with a cluster and either one or both of its children. Here, $l$ is the left pole, $r$ is the right pole, and $q$ is the query. Other points and distances are described in the text.}
    \label{fig:supplement:overlapping-children}
    \caption{CAKES uses geometric properties of clusters.}
\end{figure}


\section{Supplementary Results}

\subsection{Indexing and Tuning}

The plots in Figure~\ref{fig:supplement:indexing} show the results of these benchmarks.
The horizontal axis in each subplot shows the cardinality of the dataset augmented with synthetic points.
The left-most point on each line is at the cardinality of the original dataset without any synthetic augmentation.
The vertical axis denotes the sum of indexing and tuning time in seconds.
Both axes are on a logarithmic scale.
Hereafter, when we refer to the ``indexing time'' of an algorithm, we are implicitly referring to the sum of indexing and tuning time for said algorithm.

On all datasets, we observe that the indexing time for CAKES increases roughly linearly as cardinality increases.
HNSW and ANNOY have the slowest indexing times across all the algorithms we benchmarked for each of the four datasets, at each cardinality.
On some datasets, HNSW and ANNOY exhibit indexing times which are orders of magnitude slower than that of CAKES.
FAISS-Flat exhibits the fastest indexing time on each dataset.
This is not surprising, however, given that FAISS-Flat is a na\"{\i}ve linear search algorithm and is not building an index.

We also highlight some differences in indexing time between different datasets.
With Fashion-Mnist, as shown in Figure~\ref{fig:supplement:fashion-mnist-indexing}, we observe that the indexing time for CAKES is faster than that of FAISS-IVF for all cardinalities.
With Glove-25 (see Figure~\ref{fig:supplement:glove-25-indexing}), however, at cardinalities greater than $10^7$, FAISS-IVF has faster indexing time than CAKES.
With Sift, CAKES's indexing time is faster than that of FAISS-IVF until a cardinality of nearly $10^8$, and with the Random dataset, we observe that CAKES has faster indexing time than FAISS-IVF until a cardinality of nearly $10^7$.

\begin{figure}
    \captionsetup[subfigure]{aboveskip=-15pt,belowskip=-3pt}
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/indexing/fashion-mnist-indexing.png}\\
        \subcaption{Fashion-mnist}
        \label{fig:supplement:fashion-mnist-indexing}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/indexing/glove-25-indexing.png}\\
        \subcaption{Glove-25}
        \label{fig:supplement:glove-25-indexing}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/indexing/sift-indexing.png}\\
        \subcaption{Sift}
        \label{fig:supplement:sift-indexing}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/indexing/random-indexing.png}\\
        \subcaption{Random}
        \label{fig:supplement:random-indexing}
    \end{subfigure}%
    \\
    \caption{Indexing and tuning time for each algorithm with each of the ANN benchmark datasets and the Random dataset.}
    \label{fig:supplement:indexing}
\end{figure}

\begin{figure}
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/fashion-mnist_PermutedBall_100_throughput.png}
        \subcaption{Fashion-Mnist for $k=100$.}
        \label{fig:supplement:fashion-mnist-scaling}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/glove-25_PermutedBall_100_throughput.png}
        \subcaption{Glove-25 for $k=100$.}
        \label{fig:supplement:glove-25-scaling}
    \end{subfigure}%
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/sift_PermutedBall_100_throughput.png}
        \subcaption{Sift for $k=100$.}
        \label{fig:supplement:sift-scaling}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/random_PermutedBall_100_throughput.png}
        \subcaption{A random dataset for $k=100$.}
        \label{fig:supplement:random-scaling}
    \end{subfigure}%
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/silva-SSU-Ref_PermutedBall_100_throughput.png}
        \subcaption{Silva for $k=100$.}
        \label{fig:supplement:silva-scaling}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=1.0\textwidth]{plots/radio-ml_Ball_100_throughput.png}
        \subcaption{RadioML for $k=100$ at SnR = 10dB.}
        \label{fig:supplement:radioml-scaling}
    \end{subfigure}%
    \\
    \begin{subfigure}[b]{0.94\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{plots/legend.png}
        \label{fig:supplement:scaling-legend}
    \end{subfigure}%
    \caption{Throughput (with $k$=100) across six datasets, including a randomly-generated dataset.
    In each plot, the horizontal axis represents increasing cardinality of the dataset, while the vertical axis represents the throughput in queries per second (higher is better).
    For linear search with CAKES, we only report the throughput for a few of the initial multipliers because the trend is clear.}
    \label{fig:supplement:scaling-plots}
\end{figure}


\subsection{Clustering}

\begin{figure}
    \captionsetup[subfigure]{aboveskip=-15pt,belowskip=-3pt}
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/fashion-mnist.png}\\
        \subcaption{Fashion-mnist}
        \label{fig:supplement:fashion-mnist-radius}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/glove-25.png}\\
        \subcaption{Glove-25}
        \label{fig:supplement:glove-25-radius}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/sift.png}\\
        \subcaption{Sift}
        \label{fig:supplement:sift-radius}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/random.png}\\
        \subcaption{A random dataset}
        \label{fig:supplement:random-radius}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/silva-SSU-Ref.png}\\
        \subcaption{Silva 18S}
        \label{fig:supplement:silva-radius}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/radius/radio-ml.png}\\
        \subcaption{RadioML}
        \label{fig:supplement:radioml-radius}
    \end{subfigure}%
    \\
    \vskip 0.005in
    \begin{subfigure}[b]{0.94\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/radius/legend.png}
        \label{fig:supplement:radius-legend}
    \end{subfigure}%
    \caption{Radius vs. cluster depth across six datasets, grouped by percentile of radius and weighted by the cardinalities of the clusters.
    In order to use the same y-axis for all plots, we divided the radii of all clusters by the maximum radius of any cluster in the dataset.
    Note that for the Silva 18S and the RadioML datasets, we use a logarithmic scale for the y-axis.}
    \label{fig:supplement:radius-plots}
\end{figure}

\begin{figure}
    \captionsetup[subfigure]{aboveskip=-15pt,belowskip=-3pt}
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/fractal_density/fashion-mnist.png}\\
        \subcaption{Fashion-mnist}
        \label{fig:supplement:fashion-mnist-fractal_density}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
            \includegraphics[width=0.9\textwidth]{images/fractal_density/glove-25.png}\\
            \subcaption{Glove-25}
            \label{fig:supplement:glove-25-fractal_density}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/fractal_density/sift.png}\\
        \subcaption{Sift}
        \label{fig:supplement:sift-fractal_density}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/fractal_density/random.png}\\
        \subcaption{A random dataset}
        \label{fig:supplement:random-fractal_density}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/fractal_density/silva-SSU-Ref.png}\\
        \subcaption{Silva 18S}
        \label{fig:supplement:silva-fractal_density}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=0.9\textwidth]{images/fractal_density/radio-ml.png}\\
        \subcaption{RadioML}
        \label{fig:supplement:radioml-fractal_density}
    \end{subfigure}%
    \\
    \vskip 0.005in
    \begin{subfigure}[b]{0.94\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{images/fractal_density/legend.png}
        \label{fig:supplement:fractal_density-legend}
    \end{subfigure}%
    \caption{Fractal Density vs. cluster depth across six datasets, grouped by percentile of fractal density and weighted by the cardinalities of the clusters.
    Fractal Density is defined as $\frac{cardinality}{radius^{LFD}}$.
    As with the radius plots, we normalized the cluster radii before calculating the Fractal Density.
    We also excluded all clusters with normalized radii smaller than $10^{-6}$, as they distort the scale of the plots.}
    \label{fig:supplement:fractal_density-plots}
\end{figure}

\subsection{Clustering Strategies and Number of Distance Computations}
\label{sec:results:clustering-strategies-and-number-of-distance-computations}

In addition to the scaling experiments, we also explore how four different clustering strategies affect the performance of search.
These strategies are the Cartesian product of balanced vs. unbalanced clustering, and the presence vs. absence of depth-first reordering as described in Section~\ref{sec:methods:clustering:depth-first-reordering}.
To help with this analysis, we also added some instrumentation to our implementation of CAKES to count the number of distance computations performed during search.
We note that this instrumentation significantly slows down the wall-clock time of search, so it would not be used in a real-world application.

These four comparisons (referred to as Ball, BalancedBall, PermutedBall, and, PermutedBalancedBall, where depth-first-reordering is referred to as Permuted for brevity) for all three search algorithms on the Fashion-MNIST dataset are shown in Figure~\ref{fig:results:distance-counts}.
Notably, performance is not only strictly worse for the balanced approaches, but the asymptotic behavior is significantly worse;
the algorithms (such as Depth-First Sieve) that seemed to exhibit constant-time behavior in Figure~\ref{fig:results:scaling-plots}, which shows results using PermutedBall, no longer do so under a balanced clustering.
We note that on this dataset, depth-first reordering (the Permuted variants) seems to have little effect on throughput; depth-first reordering is primarily intended to improve space complexity.


\bibliographystyle{siamplain}
\bibliography{references}


\end{document}
